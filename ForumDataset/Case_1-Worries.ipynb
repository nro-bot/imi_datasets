{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 April 2023\n",
    "# nrobot\n",
    "# Run the acornym expansion on the full dataset!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_short, remove_stopwords, strip_multiple_whitespaces\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_page_info = ['time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "\n",
    "columns_thread_info = ['src_category_name', 'thread_page_name', 'thread_page_num', 'thread_max_pages', 'thread_page_url']\n",
    "\n",
    "columns_likes = ['num_likers', 'likers']\n",
    "columns_quotes = ['num_quotes', 'quoted_post_ids', 'quoted_authors', 'quoted_contents']\n",
    "columns_authors = ['author', 'author_title', 'author_num_posts', 'author_num_reviews', 'author_url', 'join_date_readable', 'join_date_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile='list_of_post_contents.csv', nrows=None):\n",
    "    infile='list_of_post_contents.csv'\n",
    "\n",
    "    df = pd.read_csv(Path(os.getcwd(), 'nogit_data', infile), nrows=nrows)\n",
    "    print(f'{df.columns=}')\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    df.dropna(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "    df.drop_duplicates(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    print(f'{columns_page_info=}')\n",
    "    df = df[columns_page_info]\n",
    "    df['posted_date_datetime'] = df.posted_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))\n",
    "    return df\n",
    "\n",
    "def get_discussions_only(df):\n",
    "    # remove posts that come from reviews (vs. discussions)\n",
    "    discussions = df[df.src_category_name.str.contains('Discussion')]\n",
    "    # reformat 1,000 to 1000\n",
    "    if discussions.author_num_posts.dtype != int:\n",
    "        discussions.author_num_posts = discussions.author_num_posts.apply(lambda x: x.replace(',', ''))\n",
    "        discussions.author_num_posts = discussions.author_num_posts.astype(int)\n",
    "    return discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(df):\n",
    "\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    print(my_stopwords)\n",
    "    my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "    print(f'{my_stopwords=}')\n",
    "    print(f'{df.columns=}')\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    CUSTOM_FILTERS = [\n",
    "        strip_tags, strip_punctuation, \n",
    "        lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "        lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "        lambda z: porter.stem(z, to_lowercase=False )\n",
    "    ]\n",
    "\n",
    "    df['preprocessed_posts'] = df['post_text'].parallel_apply(\n",
    "        lambda x: preprocess_string(x, CUSTOM_FILTERS)) \n",
    "    return df\n",
    "\n",
    "    #stop_nltk.extend([s.title() for s in stop_nltk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = stopwords.words('english')\n",
    "my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "porter = PorterStemmer()\n",
    "#snowball = SnowballStemmer()\n",
    "\n",
    "CUSTOM_FILTERS = [\n",
    "    strip_tags, strip_punctuation, \n",
    "    lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "    lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "    lambda z: porter.stem(z, to_lowercase=False )\n",
    "]\n",
    "preprocess_string('marry married marriage Marrying Marriage Married Marry', CUSTOM_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_corpus(df, min_count=2, threshold=10): # TODO: consider taking in a phraser model directly, instead of params to pass to one\n",
    "    posts = df.preprocessed_posts.to_list()\n",
    "    my_phrases = gensim.models.Phrases(posts, min_count=2, threshold=threshold)\n",
    "    bigram_ifier = Phraser(my_phrases)\n",
    "\n",
    "    df['bigrammed_posts'] = df['preprocessed_posts'].parallel_apply(\n",
    "        lambda post: bigram_ifier[post]) \n",
    "\n",
    "    bigrammed_corpus = df.bigrammed_posts.to_list()\n",
    "    print(f'Created word vectors for corpus size {len(bigrammed_corpus)=}, '\n",
    "          f'example post {bigrammed_corpus[0]=}')\n",
    "    return bigrammed_corpus\n",
    "    \n",
    "def create_word2vec(corpus, overwrite=False, outfile='nogit_data/Case_1/word2vec.bigrams.model', seed=None):\n",
    "    model = Word2Vec(corpus, seed=seed, workers=10)\n",
    "\n",
    "    if overwrite:\n",
    "        # Open \"path\" for writing, creating any parent directories as needed.\n",
    "        # TODO: catch exception where parent folder doesn't exist; or \n",
    "        # force it to exist by committing to git (normally nogit_data ignored by git)\n",
    "        # TODO os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "        outfile = Path(outfile)\n",
    "        if seed:\n",
    "            # add in seed to filename\n",
    "            path, file, ext = outfile.parent, outfile.stem, outfile.suffix\n",
    "            outfile = Path(path, f'{file}_seed-{seed}{ext}')\n",
    "        print(f'\\-- {overwrite=}, saved model to {outfile=}')\n",
    "        model.save(str(outfile))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = load_data()#nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df\n",
    "#df = load_data(nrows=1000)\n",
    "#df = load_data(nrows=10000)\n",
    "#df = load_data(None)d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = nltk_preprocess(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir nogit_data/Case_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df.to_pickle(f'nogit_data/Case_1/df.pd_{pd.__version__}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "bigrammed_corpus = create_bigram_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "seeds = [1,42,100,12345,888]\n",
    "models = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'{\"-\"* 40}')\n",
    "    print(f'{seed=}\\n')\n",
    "    \n",
    "    model = create_word2vec(df.preprocessed_posts, overwrite=False, seed=seed)\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('module://ipympl.backend_nbagg')\n",
    "%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non brigram \n",
    "results = {}\n",
    "\n",
    "for idx in range(5):\n",
    "    model = models[idx]\n",
    "    words = ['worries', 'worrying', 'guilty',  'family', 'Wife', 'SO','partner', 'married',  'LEO', 'law', 'police', 'trafficking', 'arrest']\n",
    "    for query in words:\n",
    "        sims = model.wv.most_similar(query, topn=10)  # get other similar words\n",
    "        #print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "        results[query] = [ f'{word} = {vector:.2f}' for word, vector in sims]\n",
    "        #print('-')\n",
    "    \n",
    "    display(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[3]\n",
    "for query in ['paper', 'laptop', 'plant']:#, 'MMS', 'HJ', 'HE', 'BJ', 'full_menu']:\n",
    "    sims = model.wv.most_similar(query, topn=30)  # get other similar words\n",
    "    print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----> model seed 42 --->\n",
    "         word 1    word 2\n",
    "word 1   top    resuls    \n",
    "word 2   second similiar\n",
    "\n",
    "so... that's ....\n",
    "for each model... have the word\n",
    "{model: word1: []\n",
    " \n",
    "or .... \n",
    " \n",
    " result 1 - which word - which model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(12345)\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "keys= ['wife', 'worry', 'SO', 'table', 'parking', 'covid', 'LEO', 'MMS', 'herpes']\n",
    "# NUMBER IS 10\n",
    "colors = ['red', 'blue', 'gold', 'purple', 'pink', 'gray', 'teal', 'orange', 'hotpink']\n",
    "\n",
    "SEED = 88\n",
    "WORDS = 8\n",
    "\n",
    "for word in keys:\n",
    "    print(f\"Key = {word}\")\n",
    "    embeddings = [model.wv[word]]\n",
    "    words = [word]\n",
    "    for similar_word, _ in model.wv.most_similar(word, topn=WORDS):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "print(n, m, k)\n",
    "#tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32, n_jobs=-1)\n",
    "umap_2d = UMAP(n_components=2, init='spectral', random_state=SEED)\n",
    "# his means that low values of n_neighbors will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point wh, default 10\n",
    "\n",
    "#umap_3d = UMAP(n_components=3, init='spectral', random_state=0)\n",
    "embeddings_en_2d = np.array(umap_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "#embeddings_en_3d = np.array(umap_3d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 3)\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "\n",
    "# hack - why are embeddings a bit different ... \n",
    "def umap_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    figsize = (9.5,6) if (matplotlib.get_backend() == 'nbAgg') else (12,12)  # interactive plot should be smaller\n",
    "    #figsize = (9.5,6)\n",
    "    fig = plt.figure(figsize=(figsize))\n",
    "    #ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "    #colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "    #colors = matplotlib.colormaps['PiYG'](np.linspace(0, 1, len(labels)))\n",
    "    #colors = matplotlib.colormaps['Spectral'](np.linspace(0, 1, len(labels)))\n",
    "    plotted_words = []\n",
    "    for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "        #x = embeddings[:, 0]\n",
    "        #y = embeddings[:, 1]\n",
    "        #plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray', s=50)\n",
    "        for i, word in enumerate(words):\n",
    "            if word not in plotted_words:\n",
    "                x = embeddings[i, 0]\n",
    "                y = embeddings[i, 1]\n",
    "                plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray')\n",
    "                if word =='worry':\n",
    "                    print(f'{x=}, {y=}')\n",
    "                sign = -1 if np.random.random() > 0.5 else 1\n",
    "                plt.annotate(word, alpha=.8, xy=(x, y), xytext=(0,0),#(2 + rng.random()*4, sign*(4 + rng.random()*4)),\n",
    "                            textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "                plotted_words.append(word)\n",
    "    ax = plt.gca()\n",
    "    legend_without_duplicate_labels(ax)\n",
    "    #plt.legend(loc=4)\"\n",
    "    plt.title(title)\n",
    "    plt.ylabel('UMAP 1')\n",
    "    plt.xlabel('UMAP 2')\n",
    "    #plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "umap_plot_similar_words('UMAP Similar words', keys, embeddings_en_2d, word_clusters, 0.8,\n",
    "                        'similar_words.png')\n",
    "plt.savefig('umap_similar_words.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feelings = {}\n",
    "def get_affinities(feeling, nouns):\n",
    "    scores = {}\n",
    "    print()\n",
    "    for noun in nouns:\n",
    "        pair = [feeling, noun]\n",
    "        print('---', ', '.join(pair))\n",
    "        print('\\t\\t', f'{model.wv.similarity(*pair):.2f}')\n",
    "        scores[noun] = f'{model.wv.similarity(*pair):.2f}'\n",
    "    all_feelings[feeling] = scores\n",
    "    return scores\n",
    "\n",
    "nouns = ['SO', 'wife', 'marriage','friends', 'family', 'LEO', 'police', 'COVID', 'herpes', 'C19', 'arrested', 'kids']\n",
    "get_affinities('worry', nouns)\n",
    "get_affinities('afraid', nouns)\n",
    "scores= get_affinities('anxious', nouns)\n",
    "my_data = pd.DataFrame(all_feelings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.reset_index().sort_values(by='worry', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_domain = nouns\n",
    "_range = colors + ['gray', 'black', 'hotpink']\n",
    "print(list(zip(_range, _domain)))\n",
    "alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N').sort('-y'),\n",
    "    y=alt.Y('worry:Q'),\n",
    "        color=alt.Color('index').scale(domain=_domain, range=_range)\n",
    "    \n",
    ").properties(height=alt.Step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N').sort('y'),\n",
    "    y=alt.Y('afraid:Q')\n",
    ").properties(height=alt.Step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N').sort('y'),\n",
    "    y=alt.Y('anxious:Q')\n",
    ").properties(height=alt.Step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_str'] = df.preprocessed_posts.parallel_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preprocessed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "word_cloud_dict=Counter(df.preprocessed_str)\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color='white',).generate_from_frequencies(word_cloud_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.__version__\n",
    "#!pip install altair==5.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pair_list:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
