{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import forum_utils\n",
    "from importlib import reload\n",
    "reload(forum_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 April 2023\n",
    "# nrobot\n",
    "# Run the acornym expansion on the full dataset!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_short, remove_stopwords, strip_multiple_whitespaces, strip_non_alphanum\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_page_info = ['time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "\n",
    "columns_thread_info = ['src_category_name', 'thread_page_name', 'thread_page_num', 'thread_max_pages', 'thread_page_url']\n",
    "\n",
    "columns_likes = ['num_likers', 'likers']\n",
    "columns_quotes = ['num_quotes', 'quoted_post_ids', 'quoted_authors', 'quoted_contents']\n",
    "columns_authors = ['author', 'author_title', 'author_num_posts', 'author_num_reviews', 'author_url', 'join_date_readable', 'join_date_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile='list_of_post_contents.csv', nrows=None):\n",
    "    infile='list_of_post_contents.csv'\n",
    "\n",
    "    df = pd.read_csv(Path(os.getcwd(), 'nogit_data', infile), nrows=nrows)\n",
    "    print(f'{df.columns=}')\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    df.dropna(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "    df.drop_duplicates(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    print(f'{columns_page_info=}')\n",
    "    df = df[columns_page_info]\n",
    "    df['posted_date_datetime'] = df.posted_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))\n",
    "    return df\n",
    "\n",
    "def get_discussions_only(df):\n",
    "    # remove posts that come from reviews (vs. discussions)\n",
    "    discussions = df[df.src_category_name.str.contains('Discussion')]\n",
    "    # reformat 1,000 to 1000\n",
    "    if discussions.author_num_posts.dtype != int:\n",
    "        discussions.author_num_posts = discussions.author_num_posts.apply(lambda x: x.replace(',', ''))\n",
    "        discussions.author_num_posts = discussions.author_num_posts.astype(int)\n",
    "    return discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUSTOM_FILTERS = [\n",
    "    strip_tags, strip_punctuation, strip_multiple_whitespaces, \n",
    "    strip_non_alphanum,\n",
    "    lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "    lambda z: porter.stem(z, to_lowercase=False ),\n",
    "    lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "    ]\n",
    "def nltk_preprocess(df, filters=None):\n",
    "    if not filters:\n",
    "        CUSTOM_FILTERS = [\n",
    "            strip_tags, strip_punctuation, strip_multiple_whitespaces, \n",
    "            strip_non_alphanum,\n",
    "            lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "            lambda z: porter.stem(z, to_lowercase=False ),\n",
    "            lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "            ]\n",
    "    else:\n",
    "        CUSTOM_FILTERS = filters\n",
    "\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    print(my_stopwords)\n",
    "    my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "    print(f'{my_stopwords=}')\n",
    "    print(f'{df.columns=}')\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "\n",
    "    df['preprocessed_posts'] = df['post_text'].parallel_apply(\n",
    "        lambda x: preprocess_string(x, CUSTOM_FILTERS)) \n",
    "    return df\n",
    "\n",
    "    #stop_nltk.extend([s.title() for s in stop_nltk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = stopwords.words('english')\n",
    "my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "porter = PorterStemmer()\n",
    "#snowball = SnowballStemmer()\n",
    "\n",
    "preprocess_string('“Contact, us” t to To I\\'m I\"m marry married marriage Marrying Marriage Married Marry I’ve', CUSTOM_FILTERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_corpus(df, min_count=2, threshold=10): # TODO: consider taking in a phraser model directly, instead of params to pass to one\n",
    "    posts = df.preprocessed_posts.to_list()\n",
    "    my_phrases = gensim.models.Phrases(posts, min_count=2, threshold=threshold)\n",
    "    bigram_ifier = Phraser(my_phrases)\n",
    "\n",
    "    df['bigrammed_posts'] = df['preprocessed_posts'].parallel_apply(\n",
    "        lambda post: bigram_ifier[post]) \n",
    "\n",
    "    bigrammed_corpus = df.bigrammed_posts.to_list()\n",
    "    print(f'Created word vectors for corpus size {len(bigrammed_corpus)=}, '\n",
    "          f'example post {bigrammed_corpus[0]=}')\n",
    "    return bigrammed_corpus\n",
    "    \n",
    "def create_word2vec(corpus, overwrite=False, outfile='nogit_data/Case_1/word2vec.bigrams.model', seed=None):\n",
    "    model = Word2Vec(corpus, seed=seed, workers=10, vector_size=100, window=5, min_count=1)\n",
    "\n",
    "    if overwrite:\n",
    "        # Open \"path\" for writing, creating any parent directories as needed.\n",
    "        # TODO: catch exception where parent folder doesn't exist; or \n",
    "        # force it to exist by committing to git (normally nogit_data ignored by git)\n",
    "        # TODO os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "        outfile = Path(outfile)\n",
    "        if seed:\n",
    "            # add in seed to filename\n",
    "            path, file, ext = outfile.parent, outfile.stem, outfile.suffix\n",
    "            outfile = Path(path, f'{file}_seed-{seed}{ext}')\n",
    "        print(f'\\-- {overwrite=}, saved model to {outfile=}')\n",
    "        model.save(str(outfile))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df = load_data()#nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH_CASE_2 = Path('nogit_data', 'Case_2')\n",
    "tag = ''\n",
    "df = pd.read_pickle(Path(DATA_PATH_CASE_2,f'{tag}preprocessed_df.pd-1.5.1.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "df\n",
    "#df = load_data(nrows=1000)\n",
    "#df = load_data(nrows=10000)\n",
    "#df = load_data(None)d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time \n",
    "df = nltk_preprocess(df)\n",
    "display(df.preprocessed_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir nogit_data/Case_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "df.to_pickle(f'nogit_data/Case_1/df.pd_{pd.__version__}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "bigrammed_corpus = create_bigram_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reload data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path('nogit_data', 'Case_2') # it's created by acornmy file\n",
    "tag = ''\n",
    "model_bigram = Word2Vec.load(str(Path(DATA_PATH, f'{tag}bigram_word2vec.gensim-4.3.1.model')))\n",
    "print('loaded word2vec model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import matplotlib\n",
    "#matplotlib.use('module://ipympl.backend_nbagg')\n",
    "#%matplotlib ipympl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# explore consisitency across seeds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## by training additional models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#seeds = [1,42,100,12345,888]\n",
    "seeds = [42]\n",
    "models = []\n",
    "\n",
    "for seed in seeds:\n",
    "    print(f'{\"-\"* 40}')\n",
    "    print(f'{seed=}\\n')\n",
    "    \n",
    "    model = create_word2vec(df.preprocessed_posts, overwrite=False, seed=seed)\n",
    "    models.append(model)\n",
    "\n",
    "model_unigram = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unigram.save(str(pathlib.Path('nogit_data', 'Case_1', f'word2vec_unigram_seed-{seed}.model')))\n",
    "model_unigram.wv.key_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unigram.wv.most_similar('im')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for non brigram \n",
    "results = {}\n",
    "\n",
    "for idx in range(5):\n",
    "    model = models[idx]\n",
    "    words = ['worries', 'worrying', 'guilty',  'family', 'Wife', 'SO','partner', 'married',  'LEO', 'law', 'police', 'trafficking', 'arrest']\n",
    "    for query in words:\n",
    "        sims = model.wv.most_similar(query, topn=10)  # get other similar words\n",
    "        #print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "        results[query] = [ f'{word} = {vector:.2f}' for word, vector in sims]\n",
    "        #print('-')\n",
    "    \n",
    "    display(pd.DataFrame(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models[3]\n",
    "for query in ['paper', 'laptop', 'plant']:#, 'MMS', 'HJ', 'HE', 'BJ', 'full_menu']:\n",
    "    sims = model.wv.most_similar(query, topn=30)  # get other similar words\n",
    "    print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "    print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "----> model seed 42 --->\n",
    "         word 1    word 2\n",
    "word 1   top    resuls    \n",
    "word 2   second similiar\n",
    "\n",
    "so... that's ....\n",
    "for each model... have the word\n",
    "{model: word1: []\n",
    " \n",
    "or .... \n",
    " \n",
    " result 1 - which word - which model\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## note: this section abandoned because i would need to subsample the data to plot the umap again with the hovered words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn # NOT pip install umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives = ['anxiety', 'worry']\n",
    "nonconcern_seeds = ['parking', 'table']\n",
    "concern_seeds = ['LEO', 'STD']\n",
    "hypothesis_seeds = ['SO', 'marriage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negatives + nonconcern_seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unigram.wv.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "umap_results =  umap.UMAP().fit(model_unigram.wv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_model = len(model_unigram.wv.key_to_index)\n",
    "size_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot as uplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_ints = np.round(rng.random(1000) * size_model).astype(int)\n",
    "random_ints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.plot\n",
    "words = model.wv.key_to_index\n",
    "#words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### failed attempt to plot umap embeddings with hover text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_words = {v: k for k, v in words.items()}\n",
    "umap.plot.output_notebook()\n",
    "p = umap.plot.interactive(umap_results, hover_data=flipped_words, point_size=2)\n",
    "umap.plot.show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### non interactive plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap.umap_ as umap\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 MAy 2023 final plot for paper "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(42)\n",
    "\n",
    "embedding_clusters = []\n",
    "word_clusters = []\n",
    "keys = negatives + nonconcern_seeds + concern_seeds +hypothesis_seeds + ['MMS', 'provider']\n",
    "# NUMBER IS 10\n",
    "colors = ['red', 'red', 'gold', 'gold', 'gray', 'gray', 'purple', 'purple', 'darkgreen', 'darkgreen']\n",
    "colors = ['red', 'red', 'goldenrod', 'goldenrod', 'darkslateblue', 'darkslateblue', 'purple', 'purple', 'forestgreen', 'forestgreen']\n",
    "#colors = ['red', 'blue', 'gold', 'purple', 'pink', 'gray', 'teal', 'orange', 'hotpink']\n",
    "\n",
    "SEED = 42\n",
    "NUM_WORDS = 4\n",
    "UMAP_N =10 \n",
    "\n",
    "for word in keys:\n",
    "    print(f\"Key = {word}\")\n",
    "    embeddings = [model_unigram.wv[word]]\n",
    "    words = [word]\n",
    "    for similar_word, _ in model_unigram.wv.most_similar(word, topn=NUM_WORDS):\n",
    "        words.append(similar_word)\n",
    "        embeddings.append(model_unigram.wv[similar_word])\n",
    "    embedding_clusters.append(embeddings)\n",
    "    word_clusters.append(words)\n",
    "    \n",
    "embedding_clusters = np.array(embedding_clusters)\n",
    "n, m, k = embedding_clusters.shape\n",
    "print(n, m, k)\n",
    "#tsne_model_unigram_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32, n_jobs=-1)\n",
    "\n",
    "# looks petty random with n_neighbors=20, looks nice at 67 8\n",
    "umap_2d = umap.UMAP(n_neighbors=UMAP_N, n_components=2, init='spectral', random_state=SEED)\n",
    "# his means that low values of n_neighbors will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point wh, default 10\n",
    "\n",
    "#umap_3d = umap.UMAP(n_neighbors=UMAP_N, n_components=3, init='spectral', random_state=SEED)\n",
    "embeddings_en_2d = np.array(umap_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "#embeddings_en_3d = np.array(umap_3d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 3)\n",
    "\n",
    "def legend_without_duplicate_labels(ax):\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "    ax.legend(*zip(*unique))\n",
    "\n",
    "# hack - why are embeddings a bit different ... \n",
    "def umap_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "    for _sentiment in negatives:\n",
    "        figsize = (9.5,6) if (matplotlib.get_backend() == 'nbAgg') else (8,7)  # interactive plot should be smaller\n",
    "        #figsize = (9.5,6)\n",
    "        fig = plt.figure(figsize=(figsize))\n",
    "        #ax = fig.add_subplot(projection='3d')\n",
    "        plotted_words = []\n",
    "        for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "            #x = embeddings[:, 0]\n",
    "            #y = embeddings[:, 1]\n",
    "            #plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray', s=50)\n",
    "            for i, word in enumerate(words):\n",
    "                if word not in plotted_words:\n",
    "                    x = embeddings[i, 0]\n",
    "                    y = embeddings[i, 1]\n",
    "                    plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray')\n",
    "                    if word =='worry':\n",
    "                        print(f'{x=}, {y=}')\n",
    "                    sign = -1 if np.random.random() > 0.5 else 1\n",
    "                    if word in keys:\n",
    "                        dist = model_unigram.wv.similarity(_sentiment, word)\n",
    "                        plt.annotate(f'{word}, {dist:.2f}', alpha=.8, xy=(x, y), xytext=(0,0),#(2 + rng.random()*4, sign*(4 + rng.random()*4)),\n",
    "                                    textcoords='offset points', ha='right', va='bottom', size=10,  weight='bold',)\n",
    "                    else:\n",
    "                        plt.annotate(f'{word}', alpha=.8, xy=(x, y), xytext=(0,0),#(2 + rng.random()*4, sign*(4 + rng.random()*4)),\n",
    "                                    textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "                    plotted_words.append(word)\n",
    "        ax = plt.gca()\n",
    "        legend_without_duplicate_labels(ax)\n",
    "        #plt.legend(loc=4)\"\n",
    "        # subtitle\n",
    "        #plt.title(f'\\nLabelled with cosine distance of keyword to \"{_sentiment}\"', fontsize=15, y=1)\n",
    "        #plt.suptitle(title, fontsize=20)\n",
    "        plt.title(title + f'\\n(Labelled with cosine distance of keyword to \"{_sentiment}\")')\n",
    "        plt.ylabel('UMAP 1')\n",
    "        plt.xlabel('UMAP 2')\n",
    "        #plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        fig.patch.set_facecolor('#F9F3DC')\n",
    "        plt.show()\n",
    "\n",
    "umap_plot_similar_words('UMAP Projection of Word2Vec Model', keys, embeddings_en_2d, word_clusters, 0.8,\n",
    "                        'similar_words.png')\n",
    "plt.savefig('umap_similar_words.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unigram = Word2Vec.load(str(pathlib.Path('nogit_data', 'Case_1', f'word2vec_unigram_seed-{seed}.model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import umap.umap_ as umap\n",
    "def plot_model(model):\n",
    "    rng = np.random.default_rng(12345)\n",
    "\n",
    "    embedding_clusters = []\n",
    "    word_clusters = []\n",
    "    keys= ['wife', 'worry', 'SO', 'table', 'parking', 'covid', 'LEO', 'MMS', 'herpes']\n",
    "    # NUMBER IS 10\n",
    "    colors = ['red', 'blue', 'gold', 'purple', 'pink', 'gray', 'teal', 'orange', 'hotpink']\n",
    "\n",
    "    SEED = 88\n",
    "    WORDS = 8\n",
    "\n",
    "    for word in keys:\n",
    "        print(f\"Key = {word}\")\n",
    "        embeddings = [model.wv[word]]\n",
    "        words = [word]\n",
    "        for similar_word, _ in model.wv.most_similar(word, topn=WORDS):\n",
    "            words.append(similar_word)\n",
    "            embeddings.append(model.wv[similar_word])\n",
    "        embedding_clusters.append(embeddings)\n",
    "        word_clusters.append(words)\n",
    "        \n",
    "    embedding_clusters = np.array(embedding_clusters)\n",
    "    n, m, k = embedding_clusters.shape\n",
    "    print(n, m, k)\n",
    "    #tsne_model_en_2d = TSNE(perplexity=15, n_components=2, init='pca', n_iter=3500, random_state=32, n_jobs=-1)\n",
    "    umap_2d = umap.UMAP(n_neighbors=10, n_components=2, init='spectral', random_state=SEED)\n",
    "    # his means that low values of n_neighbors will force UMAP to concentrate on very local structure (potentially to the detriment of the big picture), while large values will push UMAP to look at larger neighborhoods of each point wh, default 10\n",
    "\n",
    "    #umap_3d = UMAP(n_components=3, init='spectral', random_state=0)\n",
    "    embeddings_en_2d = np.array(umap_2d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 2)\n",
    "    #embeddings_en_3d = np.array(umap_3d.fit_transform(embedding_clusters.reshape(n * m, k))).reshape(n, m, 3)\n",
    "\n",
    "    def legend_without_duplicate_labels(ax):\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        unique = [(h, l) for i, (h, l) in enumerate(zip(handles, labels)) if l not in labels[:i]]\n",
    "        ax.legend(*zip(*unique))\n",
    "\n",
    "    # hack - why are embeddings a bit different ... \n",
    "    def umap_plot_similar_words(title, labels, embedding_clusters, word_clusters, a, filename=None):\n",
    "        figsize = (9.5,6) if (matplotlib.get_backend() == 'nbAgg') else (12,12)  # interactive plot should be smaller\n",
    "        #figsize = (9.5,6)\n",
    "        fig = plt.figure(figsize=(figsize))\n",
    "        #ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "        #colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
    "        #colors = matplotlib.colormaps['PiYG'](np.linspace(0, 1, len(labels)))\n",
    "        #colors = matplotlib.colormaps['Spectral'](np.linspace(0, 1, len(labels)))\n",
    "        plotted_words = []\n",
    "        for label, embeddings, words, color in zip(labels, embedding_clusters, word_clusters, colors):\n",
    "            #x = embeddings[:, 0]\n",
    "            #y = embeddings[:, 1]\n",
    "            #plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray', s=50)\n",
    "            for i, word in enumerate(words):\n",
    "                if word not in plotted_words:\n",
    "                    x = embeddings[i, 0]\n",
    "                    y = embeddings[i, 1]\n",
    "                    plt.scatter(x, y, c=[color], alpha=a, label=label, edgecolors='gray')\n",
    "                    if word =='worry':\n",
    "                        print(f'{x=}, {y=}')\n",
    "                    sign = -1 if np.random.random() > 0.5 else 1\n",
    "                    plt.annotate(word, alpha=.8, xy=(x, y), xytext=(0,0),#(2 + rng.random()*4, sign*(4 + rng.random()*4)),\n",
    "                                textcoords='offset points', ha='right', va='bottom', size=10)\n",
    "                    plotted_words.append(word)\n",
    "        ax = plt.gca()\n",
    "        legend_without_duplicate_labels(ax)\n",
    "        #plt.legend(loc=4)\"\n",
    "        plt.title(title)\n",
    "        plt.ylabel('UMAP 1')\n",
    "        plt.xlabel('UMAP 2')\n",
    "        #plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "    umap_plot_similar_words('UMAP Similar words', keys, embeddings_en_2d, word_clusters, 0.8,\n",
    "                            'similar_words.png')\n",
    "    plt.savefig('umap_similar_words.png')\n",
    "    return fig \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_model(model)\n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# against two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.wv.similarity('hi', 'hello'))\n",
    "print(model.wv.similarity('hi', 'table'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_affinities(feeling, nouns):\n",
    "    scores = {}\n",
    "    print()\n",
    "    for noun in nouns:\n",
    "        pair = [feeling, noun]\n",
    "        print('---', ', '.join(pair))\n",
    "        print('\\t\\t', f'{model.wv.similarity(*pair):.2f}')\n",
    "        scores[noun] = f'{model.wv.similarity(*pair):.2f}'\n",
    "    return scores \n",
    "\n",
    "nouns = ['SO', 'wife', 'marriage','friends', 'family', 'LEO', 'police', 'COVID', 'herpes', 'C19', 'arrested', 'kids']\n",
    "nouns = nonconcern_seeds + concern_seeds + hypothesis_seeds + ['MMS', 'provider']\n",
    "get_affinities('worry', nouns)\n",
    "get_affinities('afraid', nouns)\n",
    "all_feelings = {}\n",
    "for sentiment in negatives:\n",
    "    all_feelings[sentiment] = get_affinities(sentiment, nouns)\n",
    "\n",
    "my_data = pd.DataFrame(all_feelings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.reset_index()\n",
    "my_data['color'] = _range\n",
    "my_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data.reset_index().sort_values(by='anxiety', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(alt.__version__ )# must be at least 5.0.0 or else \"method based syntax\" does not work (for setting title)     \n",
    "# As described in the release notes for Vega-Lite 5.0.0, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment in negatives: \n",
    "    print(sentiment)\n",
    "    #_sort = my_data[sentiment].sort_values(ascending=False).values\n",
    "    #print(_sort)\n",
    "    colors = ['gold', 'gold', 'gray', 'gray', 'purple', 'purple', 'darkgreen', 'darkgreen']\n",
    "    _domain = nouns \n",
    "    _range = colors #+ ['gray', 'black', 'hotpink']\n",
    "    print(list(zip(_range, _domain)))\n",
    "    my_data = my_data.sort_values(by=sentiment, ascending=False)\n",
    "    my_data['order'] = np.arange(0, my_data.shape[0])\n",
    "    display(my_data)\n",
    "    my_chart = alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "        x=alt.X('index:N').sort('-y'),\n",
    "        #x=alt.X('index:N'),\n",
    "        #x=alt.X('index:N').sort('-y'), #sort=alt.EncodingSortField(field=f'{sentiment}:Q', op='count', order='ascending')),\n",
    "        #x=alt.X('index:N', sort=_sort), \n",
    "        y=alt.Y(f'{sentiment}:Q'),\n",
    "        #text=f'{sentiment}:Q',\n",
    "        order='order:Q', \n",
    "        color=alt.Color('index').scale(domain=_domain, range=_range)\n",
    "    ).properties(height=alt.Step(20))\n",
    "    #display(my_chart.mark_bar() + my_chart.mark_text(align='center', dy=-5))\n",
    "    #display(my_chart.mark_bar() + my_chart.mark_text())\n",
    "    display((my_chart + my_chart.mark_text().encode(\n",
    "        text=f'{sentiment}:Q',\n",
    "        order='order:Q')).resolve_scale(y='independent'))#.save('test.html')\n",
    "    display(my_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "testDf = pd.DataFrame({\n",
    "    'Country/Region': ['China', 'France', 'Italy', 'US'],\n",
    "    'Color': ['#d62728', '#d62728', '#d62728', '#1f77b4'],\n",
    "    'Total Deaths': [3180, 1266, 133, 8513],\n",
    "})\n",
    "\n",
    "bars = alt.Chart(testDf).mark_bar().encode(\n",
    "    x = 'Total Deaths',\n",
    "    y = alt.Y('Country/Region', sort='-x'),\n",
    "    color = alt.Color('Color', scale = None)\n",
    ").properties(width = 800, height = 300, title = 'Ten Countries with the Greatest Death Toll')\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align = 'left',\n",
    "    baseline = 'middle',\n",
    "    dx = 3\n",
    ").encode(\n",
    "    text = 'Total Deaths:Q',\n",
    "    y = alt.Y('Country/Region', sort='-x', axis=None),\n",
    ")\n",
    "\n",
    "(bars + text).resolve_scale(y='independent')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = my_data.reset_index().rename(columns={'index':'word'})\n",
    "test_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import pandas as pd\n",
    "\n",
    "testDf = pd.DataFrame({\n",
    "    'Country/Region': ['parking', 'table', 'leo', 'std'],\n",
    "    'Color': ['gold', 'gold', 'gray', 'gray'],\n",
    "    'Total Deaths': [0.13, 0.21, 0.47, 0.61],\n",
    "})\n",
    "\n",
    "bars = alt.Chart(testDf).mark_bar().encode(\n",
    "    y = 'Total Deaths',\n",
    "    x = alt.X('Country/Region', sort='-y'),\n",
    "    color = alt.Color('Color', scale = None)\n",
    ").properties(width = 800, height = 300, title = 'Ten Countries with the Greatest Death Toll')\n",
    "\n",
    "text = bars.mark_text(\n",
    "    align = 'left',\n",
    "    baseline = 'middle',\n",
    "    dx = 3\n",
    ").encode(\n",
    "    text = 'Total Deaths:Q',\n",
    "    x = alt.X('Country/Region', sort='-y', axis=None),\n",
    ")\n",
    "\n",
    "(bars + text).resolve_scale(x='independent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working graph!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_unigram.wv.similarity('anxiety', 'parking')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_utils.set_altair_theme()\n",
    "_range = ['gold', 'gold', 'gray', 'gray', 'purple', 'purple', 'darkgreen', 'darkgreen']\n",
    "_range = ['goldenrod', 'goldenrod', 'darkslateblue', 'darkslateblue', 'purple', 'purple', 'forestgreen', 'forestgreen']\n",
    "my_data = pd.DataFrame(all_feelings)\n",
    "my_data['color'] = _range\n",
    "#display(my_data)\n",
    "test_df = my_data.reset_index().rename(columns={'index':'word'})\n",
    "charts = []\n",
    "for sentiment in negatives:\n",
    "    title = alt.TitleParams(\n",
    "            text= 'Cosine Similarities', \n",
    "            subtitle=f'From least to most similar',\n",
    "            #subtitle=f'Between \"{sentiment}\" and Keywords',\n",
    "            align='left',\n",
    "            dy=-3\n",
    "        , anchor='start')\n",
    "    bars = alt.Chart(test_df, title=title).mark_bar(size=20).encode(\n",
    "        x=alt.X('word:N', sort='y', title='', axis=alt.Axis(labelAngle=-70)), \n",
    "        y=alt.Y(f'{sentiment}:Q', title=f'{sentiment.title()}'),\n",
    "        color = alt.Color(\n",
    "            'color:N', scale = None\n",
    "            #alt.Scale(domain=_domain, range=_range) \n",
    "        )\n",
    "    ).properties()\n",
    "    text = bars.mark_text(align='center', dy=-8,size=14).encode(\n",
    "        text=f'{sentiment}:Q',\n",
    "        x=alt.X('word:N', sort='y', axis=None),\n",
    "    )\n",
    "    #(bars + text).resolve_scale(y='independent')\n",
    "    my_chart = (bars+text).resolve_scale(x='independent').properties(\n",
    "            width=200, height=350\n",
    "        )\n",
    "    '''\n",
    "    .properties( title={ 'text': f'Cosine Similarities', 'subtitle': f'Between \"{sentiment}\" and Key Words'\n",
    "            })#.configure_title(anchor='middle')\n",
    "            \n",
    "    '''\n",
    "    charts.append(my_chart)\n",
    "    display(my_chart)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/altair-viz/altair/issues/820\n",
    "\n",
    "#x=alt.X('index:N').sort('-y'), #sort=alt.EncodingSortField(field=f'{sentiment}:Q', op='count', order='ascending')),\n",
    "# https://github.com/altair-viz/altair/issues/2013\n",
    "sentiment = 'worry'\n",
    "_range = ['darkgoldenrod', 'darkgoldenrod', 'gray', 'gray', 'purple', 'purple', 'darkgreen', 'darkgreen']\n",
    "_domain = nouns \n",
    "print(list(zip(_range, _domain)))\n",
    "sort = alt.EncodingSortField(field='worry', order='descending')\n",
    "\n",
    "bars = alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N', sort='-y'), \n",
    "    y=f'{sentiment}:Q', text=f'{sentiment}:Q',\n",
    "    color=alt.Color('index').scale(domain=_domain, range=_range)\n",
    ").properties()\n",
    "\n",
    "text = bars.mark_text().encode(\n",
    "    x=alt.X('index:N', sort='-y', axis=None),#, axis=alt.Axis(labels=False, ticks=False)),#.sort('-y'),\n",
    "    text=f'{sentiment}:Q',\n",
    ")\n",
    "\n",
    "#display(my_chart.mark_bar()+ my_chart.mark_text(align='center', dy=-5))\n",
    "#alt.layer(bars, text).resolve_scale(x='independent')\n",
    "(bars+text).resolve_scale(x='independent')\n",
    "#(bars+text).save('tmp.html')\n",
    "#display(my_chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "alt.themes.enable('none')\n",
    "\n",
    "from vega_datasets import data\n",
    "\n",
    "sort = alt.EncodingSortField(field='WW_Margin', op='median', \n",
    "                             order='descending')\n",
    "\n",
    "bars = alt.Chart().mark_bar(size=5.0).encode(\n",
    "    alt.X('Major_Genre:N', sort=sort),\n",
    "    y='q1(WW_Margin):Q',\n",
    "    y2='q3(WW_Margin):Q'\n",
    ")\n",
    "\n",
    "ticks = alt.Chart().mark_tick(\n",
    "    color='black',\n",
    "    size=5.0\n",
    ").encode(\n",
    "    alt.X('Major_Genre:N', sort=sort, axis=alt.Axis(labels=False, ticks=False)),\n",
    "    y='median(WW_Margin):Q',\n",
    ")\n",
    "\n",
    "alt.layer(bars, ticks, data=data.movies.url).transform_calculate(\n",
    "    WW_Margin = \"(datum.Worldwide_Gross - datum.Production_Budget) / 1e6\"\n",
    ").resolve_scale(x='independent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N').sort('y'),\n",
    "    y=alt.Y('afraid:Q')\n",
    ").properties(height=alt.Step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.Chart(my_data.reset_index()).mark_bar().encode(\n",
    "    x=alt.X('index:N').sort('y'),\n",
    "    y=alt.Y('anxious:Q')\n",
    ").properties(height=alt.Step(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['preprocessed_str'] = df.preprocessed_posts.parallel_apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preprocessed_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "word_cloud_dict=Counter(df.preprocessed_str)\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                    background_color='white',).generate_from_frequencies(word_cloud_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.__version__\n",
    "#!pip install altair==5.0.0rc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pair in pair_list:\n",
    "    print(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
