{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 April 2023\n",
    "# nrobot\n",
    "# acquire urls \n",
    "\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from collections import Counter\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "import os\n",
    "import datetime\n",
    "import tldextract   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "from urllib.parse import urlparse\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOURCE: from parse_links import parse_domains\n",
    "# TODO: consider replacing urlparse with tldextract ?\n",
    "# TODO: push check_dns flag back to original src file parse_links.py\n",
    "\n",
    "extractor = URLExtract()\n",
    "extractor.add_enclosure(\"(\", \")\")\n",
    "extractor.add_enclosure(\"[\", \"]\")\n",
    "\n",
    "def parse_domain(url):\n",
    "    if \"//\" not in url:\n",
    "        url = \"//\" + url\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "def parse_domains(text):\n",
    "    return list(set(parse_domain(url) for url in extractor.gen_urls(text)))\n",
    "\n",
    "# TODO: refactor the below\n",
    "def parse_domains_and_validate(text):\n",
    "    return list(set(parse_domain(url) for url in extractor.gen_urls(text, check_dns=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './nogit_data/list_of_post_contents.csv'\n",
    "df = pd.read_csv(file)#, nrows=10_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decorator that annotates filename with pandas version\n",
    "def retrieve_domains(df, outfile='nogit_data/domains.pkl', overwrite_existing=False):\n",
    "    # df - must contain 'post_text' column\n",
    "\n",
    "    # if the file exists, load it.  # if overwrite, then re-run and save it.\n",
    "    # if file not exists, run and save it.\n",
    "    # return file\n",
    "    \n",
    "    if (not os.path.exists(outfile)) or (os.path.exists(outfile) & overwrite_existing):\n",
    "        domains = df['post_text'].dropna().parallel_apply(parse_domains)\n",
    "        domains[domains.apply(bool)].to_pickle(outfile)\n",
    "    else: \n",
    "        domains = pd.read_pickle(outfile)\n",
    "    \n",
    "    # TODO: these 3 lines can definitely be one line. \n",
    "    t = os.path.getmtime(outfile)\n",
    "    t = datetime.datetime.fromtimestamp(t)\n",
    "    t = t.strftime('%H:%M:%S / %B %d %Y')\n",
    "    print(f'Loaded {outfile}, modified at {t}')\n",
    "    return domains\n",
    "\n",
    "def retrieve_domains_and_validate(df, outfile='nogit_data/domains_validated_only.pkl', overwrite_existing=False):\n",
    "    # same as above, but with check_dns = True on the parse_domain function\n",
    "    \n",
    "    if (not os.path.exists(outfile)) or (os.path.exists(outfile) & overwrite_existing):\n",
    "        domains = df['post_text'].dropna().parallel_apply(parse_domains_and_validate)\n",
    "        domains[domains.apply(bool)].to_pickle(outfile)\n",
    "    else: \n",
    "        domains = pd.read_pickle(outfile)\n",
    "    return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains = retrieve_domains(df)#, overwrite_existing=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_validated_only = retrieve_domains_and_validate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(domains.sample(n=40))\n",
    "domains_validated_only.sample(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fix url parsing, bug example:\n",
    "print(domains.loc[111371])\n",
    "print(domains_validated_only.loc[111371])\n",
    "#print(df.iloc[111371].post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_urls(df):\n",
    "    # Input: df where each row is a python list of urls\n",
    "\n",
    "    cnt = Counter()\n",
    "    all_links = []\n",
    "    for link_list in df.values:\n",
    "        all_links.extend(link_list)\n",
    "    for link in all_links: \n",
    "        cnt[link] += 1\n",
    "\n",
    "    df_links = pd.DataFrame.from_dict(cnt, orient='index').reset_index()\n",
    "    df_links.columns = ['url', 'counts']\n",
    "    df_links['normalized_counts'] = df_links['counts'] / sum(df_links['counts']) * 100\n",
    "    return df_links\n",
    "\n",
    "df_links = count_urls(domains)\n",
    "display(df_links.sort_values(by='counts', ascending=False))\n",
    "df_links.sort_values(by='counts').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to df_links, ? then count again?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(list_url):\n",
    "    return ['.'.join(tldextract.extract(url)[1:]) for url in list_url]\n",
    "no_suffix = domains.apply(extract_domain)\n",
    "display(no_suffix)\n",
    "tmp = count_urls(no_suffix)\n",
    "tmp\n",
    "# -- \n",
    "no_suffix = domains_validated_only.apply(extract_domain)\n",
    "display(no_suffix)\n",
    "tmp = count_urls(no_suffix)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tmp.sort_values(by='counts').iloc[:20])\n",
    "display(tmp.sort_values(by='counts').iloc[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "#tmp.sort_values(by='counts').iloc[-20:].counts.plot(kind='bar', xlabel='url')\n",
    "_tmp = count_urls(no_suffix)\n",
    "top20 = _tmp.sort_values(by='counts').iloc[-20:]\n",
    "alt.Chart(top20).mark_bar().encode(\n",
    "    x=alt.X('url', axis=alt.Axis(labelAngle=-45)),\n",
    "    y='counts:Q',\n",
    "    text='url'\n",
    ").properties(width=600, height=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
