{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 19 April 2023: \n",
    "# Disappointing update, trained on kaggle forum posts in the hopes that SO would be filled out (into significant other)\n",
    "# It definitely did worse. They've definitely cleaned NSFW from the dataset also. Alas\n",
    "# Really should expand to trigrams, but... time... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 April 2023\n",
    "# nrobot\n",
    "# Run the acornym expansion on the full dataset!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_short, remove_stopwords, strip_multiple_whitespaces\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_page_info = ['time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "\n",
    "columns_thread_info = ['src_category_name', 'thread_page_name', 'thread_page_num', 'thread_max_pages', 'thread_page_url']\n",
    "\n",
    "columns_likes = ['num_likers', 'likers']\n",
    "columns_quotes = ['num_quotes', 'quoted_post_ids', 'quoted_authors', 'quoted_contents']\n",
    "columns_authors = ['author', 'author_title', 'author_num_posts', 'author_num_reviews', 'author_url', 'join_date_readable', 'join_date_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile='nogit_data/list_of_post_contents.csv', nrows=None):\n",
    "    # read, drop duplicates and NaNs in post_text, and convert post date string to a DateTime object\n",
    "\n",
    "    df = pd.read_csv(infile, nrows=nrows)\n",
    "    print(f'{df.columns=}')\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    df.dropna(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "    df.drop_duplicates(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    print(f'{columns_page_info=}')\n",
    "    df = df[columns_page_info]\n",
    "    df['posted_date_datetime'] = df.posted_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))\n",
    "    return df\n",
    "\n",
    "def get_discussions_only(df):\n",
    "    # remove posts that come from reviews (vs. discussions)\n",
    "    discussions = df[df.src_category_name.str.contains('Discussion')]\n",
    "    # reformat 1,000 to 1000\n",
    "    if discussions.author_num_posts.dtype != int:\n",
    "        discussions.author_num_posts = discussions.author_num_posts.apply(lambda x: int(x.replace(',', '')))\n",
    "    return discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(df, text_col_name='post_text', print_more=False):\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    if print_more:\n",
    "        print(my_stopwords)\n",
    "    my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "    if print_more:\n",
    "        print(f'{my_stopwords=}')\n",
    "        print(f'{df.columns=}')\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    CUSTOM_FILTERS = [\n",
    "        strip_tags, strip_punctuation, \n",
    "        lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "        lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "        lambda z: porter.stem(z, to_lowercase=False )\n",
    "    ]\n",
    "    if print_more:\n",
    "        print('pre-processing filters applied:', CUSTOM_FILTERS)\n",
    "\n",
    "    df['preprocessed_posts'] = df[text_col_name].parallel_apply(\n",
    "        lambda x: preprocess_string(x, CUSTOM_FILTERS)) \n",
    "    return df\n",
    "\n",
    "def create_bigrams(df, text_col_name='preprocessed_posts', overwrite=False, outfile='word2vec.bigrams.model'):\n",
    "    posts = df[text_col_name].to_list()\n",
    "    my_phrases = gensim.models.Phrases(posts, min_count=1, threshold=10)\n",
    "    bigram_ifier = Phraser(my_phrases)\n",
    "\n",
    "    print('creating bigrams')\n",
    "    #df['bigrammed_posts'] = df['preprocessed_posts'].parallel_apply(\n",
    "    df['bigrammed_posts'] = df[text_col_name].progress_apply(\n",
    "        lambda post: bigram_ifier[post]) \n",
    "\n",
    "    return df\n",
    "\n",
    "def create_word2vec(df, bigram_col_name='bigrammed_posts', overwrite=False, outfile=f'word2vec.bigrams.gensim-{gensim.__version__}.model'):\n",
    "    bigrammed_corpus = df[bigram_col_name].to_list()\n",
    "    print(f'Creating word vectors for corpus size {len(bigrammed_corpus)=}, '\n",
    "          f'example post {bigrammed_corpus[0]=}')\n",
    "    print('creating model now!')\n",
    "    model = Word2Vec(bigrammed_corpus, seed=42, workers=10)\n",
    "\n",
    "    if overwrite: # TODO: test if this works with the sleep fxn\n",
    "        # TODO WISHLIST: put sleep fxn into a decorator\n",
    "        # TODO WISHLIST catch exception where parent folder doesn't exist; or \n",
    "        # force it to exist by committing to git (normally nogit_data ignored by git)\n",
    "        # os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "        print(f\"\\-- {overwrite=}! Saving model to {Path('nogit_data', 'Case_1', outfile)=}\")\n",
    "        print('Do you have a bkup? about to overwrite a file of size (TBD) created on ( )')\n",
    "        sleep(1)\n",
    "        print('2 sec to cancel')\n",
    "        sleep(2)\n",
    "        model.save(Path('nogit_data', 'Case_2', outfile))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#df = load_data(nrows=1000)\n",
    "#df = load_data(nrows=10000)\n",
    "#df = load_data(None)\n",
    "\n",
    "# TODO: this makes assumptions that nogit_data is in the same folder\n",
    "DATA_PATH = Path('nogit_data', 'Case_2')\n",
    "def redo_or_load_forum(redo_steps=None, raw_csv_file=None, overwrite_results=None, sample_size=None, tag=''):\n",
    "    # redo_steps, overwrite_results should be lists\n",
    "    # 'clean_corpus', 'create_bigrams', 'create_word2vec' \n",
    "    # all filenames are relative to the data path (case_2 in nogit_data')\n",
    "    # TODO: overwerite_results must be subset of redo_steps\n",
    "    df, bigram_df, model = None, None, None\n",
    "    if redo_steps: \n",
    "        print('redoing')\n",
    "        print(f'{redo_steps=}')\n",
    "        print(f'will overwrite {overwrite_results}')\n",
    "        if 'load_data' in redo_steps: \n",
    "            '''\n",
    "            # todo... there shouldn't be a separate load_data option\n",
    "            if not raw_csv_file:\n",
    "                raw_csv_file = Path('..', 'list_of_post_contents.csv')\n",
    "            df = load_data(infile=Path(DATA_PATH, '..', raw_csv_file))#nrows=100000)\n",
    "            if 'load_data' in ovewrite_results: \n",
    "                pass # not going to bother saving\n",
    "            '''\n",
    "            pass\n",
    "        elif 'clean_corpus' in redo_steps:\n",
    "            if not raw_csv_file:\n",
    "                raw_csv_file = Path('..', 'list_of_post_contents.csv')\n",
    "            df = load_data(infile=Path(DATA_PATH, raw_csv_file))\n",
    "            df = nltk_preprocess(df)\n",
    "            if 'clean_corpus' in overwrite_results:\n",
    "                df.to_pickle(Path(DATA_PATH,f'{tag}preprocessed_df.pd-{pd.__version__}.pkl'))\n",
    "        elif 'create_bigrams' in redo_steps:\n",
    "            df = pd.read_pickle(Path(DATA_PATH,f'{tag}preprocessed_df.pd-1.5.1.pkl'))\n",
    "            bigram_df = create_bigrams(df)\n",
    "            if 'create_bigrams' in overwrite_results: \n",
    "                bigram_df.to_pickle(Path(DATA_PATH, f'{tag}bigram.df.pd-{pd.__version__}.pkl'))\n",
    "        elif 'create_word2vec' in redo_steps:\n",
    "            bigram_df = pd.read_pickle(Path(DATA_PATH, f'{tag}bigram.df.pd-1.5.1.pkl'))\n",
    "            model = create_word2vec(bigram_df)\n",
    "            print('created model')\n",
    "            if 'create_word2vec' in overwrite_results:\n",
    "                print('saving model')\n",
    "                model.save(str(Path(DATA_PATH, f'{tag}bigram_word2vec.gensim-{gensim.__version__}.model')))\n",
    "                print('saved')\n",
    "    else:\n",
    "        print('loading existing files')\n",
    "        df = pd.read_pickle(Path(DATA_PATH,f'{tag}preprocessed_df.pd-1.5.1.pkl'))\n",
    "        print('loaded preprocessed')\n",
    "        bigram_df = pd.read_pickle(Path(DATA_PATH, f'{tag}bigram.df.pd-1.5.1.pkl'))\n",
    "        print('loaded bigrams')\n",
    "        model = Word2Vec.load(str(Path(DATA_PATH, f'{tag}bigram_word2vec.gensim-4.3.1.model')))\n",
    "        print('loaded word2vec model')\n",
    "    return df, bigram_df, model\n",
    "# AttributeError: 'PosixPath' object has no attribute 'endswith'\n",
    "# NOTE: CANNOT USE Path in model.save, on string !!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.__version__\n",
    "Path('test', f'bigram_word2vec.gensim-{gensim.__version__}.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(model_mp.wv.most_similar('TS'))\n",
    "#bigram_ifier = Phraser(my_phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#df_mp, bigram_mp, model_mp = redo_or_load_forum(redo_steps=['clean_corpus', 'create_bigrams', 'create_word2vec'], overwrite_results=['clean_cocreate_bigrams', 'create_word2vec'])\n",
    "#df_mp, bigram_mp, model_mp = redo_or_load_forum(redo_steps=['create_word2vec'], overwrite_results=['create_word2vec'])\n",
    "df_mp, bigram_mp, model_mp = redo_or_load_forum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Word2Ve./nogit_data/d('nogit_data/tmp_word2vec.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = ''\n",
    "model_mp = Word2Vec.load(str(Path(DATA_PATH, f'{tag}bigram_word2vec.gensim-4.3.1.model')))\n",
    "my_model = model_mp\n",
    "for query in ['SO']:#, 'MMS', 'HJ', 'HE', 'BJ', 'full_menu']:\n",
    "  sims = my_model.wv.most_similar(query, topn=20)  # get other similar words\n",
    "  #for item in sims:\n",
    "  print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "  print('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_abbreviation(query, model=None, word_vectors=None):\n",
    "    query = query.upper()\n",
    "    if word_vectors:\n",
    "        wv = word_vectors\n",
    "    elif model:\n",
    "        wv = model.wv\n",
    "    else:\n",
    "        return('Must provide model or word_vectors')\n",
    "    try:\n",
    "        similar_words = wv.most_similar(query, topn=50)  # get other similar words\n",
    "    except KeyError as e:\n",
    "        print(f'-- {query=}', e, '(Never encountered query in training corpus)')\n",
    "        return\n",
    "    phrases = [word for word, vector in similar_words if '_' in word]\n",
    "    for phrase in phrases:\n",
    "        bigram = phrase.split('_')\n",
    "        candidate = ''.join([w[0] for w in bigram])\n",
    "        if query == candidate.upper():\n",
    "            print(f'{query=}, {phrase=}, \\t Candidate: {phrase.replace(\"_\", \" \")}')\n",
    "            return candidate\n",
    "    print(f'-- {query=}, no expanded bigram found')\n",
    "    unigrams = [word for word, vector in similar_words if '_' not in word]\n",
    "    for word in unigrams:\n",
    "        if word.upper() != query:\n",
    "            if query[0] == word[0].upper():\n",
    "                return print(f'{query=} \\t Candidate UNIgram: {word}')\n",
    "    print(f'-- {query=}, no expanded bigram NOR unigram found')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_abbreviation('TS', my_model)\n",
    "find_abbreviation('ST', my_model)\n",
    "find_abbreviation('BJ', my_model)\n",
    "find_abbreviation('HE', my_model)\n",
    "find_abbreviation('HJ', my_model)\n",
    "find_abbreviation('FS', my_model)\n",
    "find_abbreviation('SO', my_model)\n",
    "find_abbreviation('CMT', my_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# print more detailed results, for acronyms that failed  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = my_model.wv.most_similar('10')\n",
    "a\n",
    "dict(a).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcy import silent\n",
    "from collections import OrderedDict\n",
    "\n",
    "def print_more(acronym, model=None, word_vectors=None, scores=False):\n",
    "    if word_vectors:\n",
    "        wv = word_vectors\n",
    "    elif model:\n",
    "        wv = model.wv\n",
    "    else:\n",
    "        return('Must provide model or word_vectors')\n",
    "\n",
    "    similar_words = wv.most_similar(acronym, topn=100)  # get other similar words\n",
    "    similar_words = dict(similar_words) \n",
    "    \n",
    "    phrases = [word for word in similar_words.keys() if '_' in word]\n",
    "    unigrams = [word for word in similar_words.keys() if '_' not in word]\n",
    "    \n",
    "    result = None\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split('_')\n",
    "        init_letters = [w[0] for w in words]\n",
    "        candidate = ''.join(init_letters).upper()\n",
    "        if acronym.upper() == candidate:\n",
    "            result = candidate\n",
    "            break\n",
    "    if result:\n",
    "        print(f'{acronym=}, Candidate: {phrase.replace(\"_\", \" \")}')\n",
    "    else:\n",
    "        print(f'{acronym=}, no candidate found')\n",
    "        \n",
    "    print()\n",
    "    #print(f'{similar_words=}')\n",
    "    print()\n",
    "    if scores:\n",
    "        display([f'{phrase}    {similar_words[phrase]:.2f}' for phrase in phrases[:5]])\n",
    "        print()\n",
    "        display([f'{word}     {similar_words[word]:.2f}' for word in unigrams[:5]])\n",
    "    else:\n",
    "        print(f'{acronym=}')\n",
    "        display(phrases[:5])\n",
    "        display(unigrams[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'TS'.split('_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_more('ST', my_model, scores=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_more('CMT', my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_abbreviation('RM', my_model)\n",
    "find_abbreviation('CL', my_model)\n",
    "find_abbreviation('TER', my_model)\n",
    "\n",
    "find_abbreviation('CK', my_model) # not found in dictionary\n",
    "find_abbreviation('CL', my_model) # not found in dictionary\n",
    "\n",
    "find_abbreviation('LE', my_model)\n",
    "find_abbreviation('CC', my_model)\n",
    "\n",
    "find_abbreviation('HE', my_model)\n",
    "find_abbreviation('FS', my_model)\n",
    "find_abbreviation('HJ', my_model)\n",
    "find_abbreviation('MP', my_model)\n",
    "find_abbreviation('MMS',my_model)\n",
    "find_abbreviation('SO', my_model) # not found in dictionary\n",
    "\n",
    "find_abbreviation('MT', my_model)\n",
    "# find_abbreviation('PV', my_model) # not found in dictionary\n",
    "#print_more('TS', my_model) # not found in dictionary\n",
    "find_abbreviation('SG', my_model) # not found in dictionary\n",
    "find_abbreviation('SP', my_model) # not found in dictionary\n",
    "find_abbreviation('TG', my_model) # not found in dictionary\n",
    "find_abbreviation('TV', my_model) # not found in dictionary\n",
    "find_abbreviation('WF', my_model) # not found in dictionary\n",
    "find_abbreviation('WG', my_model) # not found in dictionary\n",
    "\n",
    "find_abbreviation('FJ', my_model)\n",
    "find_abbreviation('DP', my_model)\n",
    "print_more('CL', my_model) # not found in dictionary # craiglist is one word\n",
    "print_more('PS', my_model)\n",
    "#print_more('BS', my_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rg \"significant other\" nogit_data/Case_2/KaggleForumMessages.csv \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# compared to pretreained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gensim.downloader\n",
    "# more details about the models are at https://github.com/RaRe-Technologies/gensim-data\n",
    "#gnew_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "gnew_vectors = gensim.downloader.load('glove-twitter-25')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "find_abbreviation('lol', word_vectors=gnew_vectors)\n",
    "print_more('sw', word_vectors=gnew_vectors)\n",
    "# oh... they don't have bigrams in them ... \n",
    "# initialism\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pretrained + fineturned on my dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_posts = pd.read_csv(Path(DATA_PATH, 'KaggleForumMessages.csv'))\n",
    "forum_posts['Message'] = forum_posts.Message.fillna('')\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_posts[forum_posts.Message.isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_posts['post_text'] = forum_posts['Message'].progress_apply(lambda x: BeautifulSoup(x, \"lxml\").text)\n",
    "# parallel apply may choke silently (make no progress) if underlying exception thrown\n",
    "forum_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forum_posts.to_pickle(Path(DATA_PATH,'kaggle_stripped_html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False: \n",
    "    kaggle_df = nltk_preprocess(forum_posts)\n",
    "    kaggle_df.to_pickle(Path(DATA_PATH,'kaggle_nltk_processed.df.pkl'))\n",
    "    bigram_kaggle = create_bigrams(kaggle_df)\n",
    "    bigram_kaggle.to_pickle(Path(DATA_PATH,'kaggle_bigram.df.pkl'))\n",
    "    model_kaggle = create_word2vec(bigram_kaggle)\n",
    "    model_kaggle.save(str(Path(DATA_PATH,'kaggle.model')))\n",
    "model_kaggle = Word2Vec.load(str(Path(DATA_PATH,'kaggle.model')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#wiki_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "'''\n",
    "redo_or_load(redo_steps=['clean_corpus', 'create_bigrams', 'create_word2vec'], \n",
    "             overwrite_results=['clean_corpus', 'create_bigrams', 'create_word2vec'],\n",
    "             raw_csv_file='KaggleForumMessages.csv',\n",
    "             tag='kaggle-'\n",
    "            )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcy import silent\n",
    "my_model = model_kaggle \n",
    "def tmp_wrapper():\n",
    "    find_abbreviation('CC', my_model)\n",
    "    find_abbreviation('HE', my_model)\n",
    "    find_abbreviation('FS', my_model)\n",
    "    find_abbreviation('LE', my_model)\n",
    "    find_abbreviation('MP', my_model)\n",
    "    find_abbreviation('MT', my_model)\n",
    "    # find_abbreviation('PV', my_model) # not found in dictionary\n",
    "    print_more('SO', my_model) # not found in dictionary\n",
    "    find_abbreviation('SG', my_model) # not found in dictionary\n",
    "    find_abbreviation('SO', my_model) # not found in dictionary\n",
    "    find_abbreviation('SP', my_model) # not found in dictionary\n",
    "    find_abbreviation('TV', my_model) # not found in dictionary\n",
    "    find_abbreviation('WF', my_model) # not found in dictionary\n",
    "    find_abbreviation('WG', my_model) # not found in dictionary\n",
    "    find_abbreviation('HJ', my_model)\n",
    "\n",
    "    find_abbreviation('FJ', my_model)\n",
    "    find_abbreviation('DP', my_model)\n",
    "    print_more('CL', my_model) # not found in dictionary # craiglist is one word\n",
    "    print_more('PS', my_model)\n",
    "    #print_more('BS', my_model)\n",
    "    find_abbreviation('TG', my_model) # not found in dictionary\n",
    "tmp_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialisms = ['FS', 'HE', 'LE', 'ST', 'TS', 'CL', 'RM', 'BP'] \n",
    "my_model = model_mp\n",
    "for abbr in initialisms:\n",
    "    print(find_abbreviation(abbr, my_model))\n",
    "    print(print_more(abbr, my_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
