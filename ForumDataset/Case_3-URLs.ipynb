{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14 April 2023\n",
    "# nrobot\n",
    "# acquire urls \n",
    "\n",
    "import pandas as pd\n",
    "from pandarallel import pandarallel\n",
    "from collections import Counter\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "import os\n",
    "import datetime\n",
    "import tldextract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urlextract import URLExtract\n",
    "from urllib.parse import urlparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CODE SOURCE: from parse_links import parse_domains\n",
    "# TODO: consider replacing urlparse with tldextract ?\n",
    "# TODO: push check_dns flag back to original src file parse_links.py\n",
    "\n",
    "extractor = URLExtract()\n",
    "extractor.add_enclosure(\"(\", \")\")\n",
    "extractor.add_enclosure(\"[\", \"]\")\n",
    "\n",
    "def parse_domain(url):\n",
    "    if \"//\" not in url:\n",
    "        url = \"//\" + url\n",
    "    return urlparse(url).netloc\n",
    "\n",
    "def parse_domains(text):\n",
    "    return list(set(parse_domain(url) for url in extractor.gen_urls(text)))\n",
    "\n",
    "# TODO: refactor the below\n",
    "def parse_domains_and_validate(text):\n",
    "    return list(set(parse_domain(url) for url in extractor.gen_urls(text, check_dns=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extractor.get_enclosures())\n",
    "print(extractor.get_stop_chars_left())\n",
    "print(extractor.get_stop_chars_right())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './nogit_data/list_of_post_contents.csv'\n",
    "df = pd.read_csv(file)#, nrows=10_00)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[273225].post_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample(10).post_text.dropna().apply(parse_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: decorator that annotates filename with pandas version\n",
    "def retrieve_domains(df, outfile='nogit_data/domains.pkl', overwrite_existing=False):\n",
    "    # df - must contain 'post_text' column\n",
    "\n",
    "    # if the file exists, load it.  # if overwrite, then re-run and save it.\n",
    "    # if file not exists, run and save it.\n",
    "    # return file\n",
    "    \n",
    "    if (not os.path.exists(outfile)) or (os.path.exists(outfile) & overwrite_existing):\n",
    "        domains = df['post_text'].dropna().parallel_apply(parse_domains)\n",
    "        domains[domains.apply(bool)].to_pickle(outfile)\n",
    "    else: \n",
    "        domains = pd.read_pickle(outfile)\n",
    "    \n",
    "    # TODO: these 3 lines can definitely be one line. \n",
    "    t = os.path.getmtime(outfile)\n",
    "    t = datetime.datetime.fromtimestamp(t)\n",
    "    t = t.strftime('%H:%M:%S / %B %d %Y')\n",
    "    print(f'Loaded {outfile}, modified at {t}')\n",
    "    return domains\n",
    "\n",
    "def retrieve_domains_and_validate(df, outfile='nogit_data/domains_validated_only.pkl', overwrite_existing=False):\n",
    "    # same as above, but with check_dns = True on the parse_domain function\n",
    "    \n",
    "    if (not os.path.exists(outfile)) or (os.path.exists(outfile) & overwrite_existing):\n",
    "        domains = df['post_text'].dropna().parallel_apply(parse_domains_and_validate)\n",
    "        domains[domains.apply(bool)].to_pickle(outfile)\n",
    "    else: \n",
    "        domains = pd.read_pickle(outfile)\n",
    "    return domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "DATA_PATH = Path('nogit_data/Case_2')\n",
    "tag = ''\n",
    "df = pd.read_pickle(Path(DATA_PATH,f'{tag}preprocessed_df.pd-1.5.1.pkl'))\n",
    "posts = df[['posted_date_datetime', 'post_text']].copy()\n",
    "posts['domains'] = retrieve_domains_and_validate(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_urls(df):\n",
    "    # Input: df where each row is a python list of urls\n",
    "\n",
    "    cnt = Counter()\n",
    "    all_links = []\n",
    "    for link_list in df.values:\n",
    "        all_links.extend(link_list)\n",
    "    for link in all_links: \n",
    "        cnt[link] += 1\n",
    "\n",
    "    df_links = pd.DataFrame.from_dict(cnt, orient='index').reset_index()\n",
    "    df_links.columns = ['url', 'counts']\n",
    "    df_links['normalized_counts'] = df_links['counts'] / sum(df_links['counts']) * 100\n",
    "    return df_links\n",
    "\n",
    "def extract_domain(list_url):\n",
    "    return ['.'.join(tldextract.extract(url)[1:]) for url in list_url]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts.domains.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = [(2018, 2020), (2021, 2023)]\n",
    "for year0, year1 in years:\n",
    "    print('year', year0, year1)\n",
    "    date_filter = (posts.posted_date_datetime >= f'{year0}-01-01') \\\n",
    "        & (posts.posted_date_datetime <= f'{year1}-12-31')\n",
    "    domains = posts[date_filter].domains.dropna()\n",
    "    domains = domains.apply(extract_domain)\n",
    "    counts = count_urls(domains)\n",
    "    display(counts.sort_values(by='counts', ascending=False).iloc[:20])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domains_validated_only = retrieve_domains_and_validate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(domains.sample(n=40))\n",
    "domains_validated_only.sample(n=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO - fix url parsing, bug example:\n",
    "print(domains.loc[111371])\n",
    "print(domains_validated_only.loc[111371])\n",
    "#print(df.iloc[111371].post_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_urls(df):\n",
    "    # Input: df where each row is a python list of urls\n",
    "\n",
    "    cnt = Counter()\n",
    "    all_links = []\n",
    "    for link_list in df.values:\n",
    "        all_links.extend(link_list)\n",
    "    for link in all_links: \n",
    "        cnt[link] += 1\n",
    "\n",
    "    df_links = pd.DataFrame.from_dict(cnt, orient='index').reset_index()\n",
    "    df_links.columns = ['url', 'counts']\n",
    "    df_links['normalized_counts'] = df_links['counts'] / sum(df_links['counts']) * 100\n",
    "    return df_links\n",
    "\n",
    "df_links = count_urls(domains)\n",
    "display(df_links.sort_values(by='counts', ascending=False))\n",
    "df_links.sort_values(by='counts').iloc[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to df_links, ? then count again?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain(list_url):\n",
    "    return ['.'.join(tldextract.extract(url)[1:]) for url in list_url]\n",
    "no_suffix = domains.apply(extract_domain)\n",
    "display(no_suffix)\n",
    "tmp = count_urls(no_suffix)\n",
    "tmp\n",
    "# -- \n",
    "no_suffix = domains_validated_only.apply(extract_domain)\n",
    "display(no_suffix)\n",
    "tmp = count_urls(no_suffix)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp[tmp.url == 'twitter.com']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tmp.sort_values(by='counts').iloc[:20])\n",
    "display(tmp.sort_values(by='counts').iloc[-20:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import altair as alt\n",
    "#tmp.sort_values(by='counts').iloc[-20:].counts.plot(kind='bar', xlabel='url')\n",
    "_tmp = count_urls(no_suffix)\n",
    "top20 = _tmp.sort_values(by='counts').iloc[-20:]\n",
    "chart = alt.Chart(top20).mark_bar(size=20).encode(\n",
    "    x=alt.X('url', axis=alt.Axis(labelAngle=-30)).sort('-y'),\n",
    "    y=alt.Y('normalized_counts:Q', title='% of all URLs'),\n",
    "    text='url'\n",
    ").properties(width=600, height=200, title='Most Popular URLs (by domain)')\n",
    "display(chart)\n",
    "display(top20.iloc[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.__version__ \n",
    "SCALE=1.5\n",
    "# TODO: refactor into forum_utils.py\n",
    "def theme_1(*args, **kwargs):\n",
    "    return {'width': 400, 'height': 300,\n",
    "            'config': {#'style': {'bar': {'size': 20}},\n",
    "                       #'legend': {'symbolSize': 20, 'titleFontSize': 20, 'labelFontSize': 20}, \n",
    "                       'title':{'fontSize': 20*SCALE},\n",
    "                       'axis': {'titleFontSize': 15*SCALE, 'labelFontSize': 11*SCALE},\n",
    "                        \"axisX\": { #https://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602\n",
    "                                \"tickSize\": 10, # default, including it just to show you can change it\n",
    "                                \"tickWidth\":2,\n",
    "                        }\n",
    "            },\n",
    "    }\n",
    "            #'encoding': {'x': {'axis': {'title': 'options'}, 'scale': {'paddingOuter': 0.5, 'paddingInner': 0.5}},\n",
    "            #             'y': {'axis': {'title': 'percentage of students'}}}}\n",
    "alt.themes.register('theme_1', theme_1)\n",
    "alt.themes.enable('theme_1')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "tags": []
   },
   "source": [
    "import altair as alt\n",
    "#tmp.sort_values(by='counts').iloc[-20:].counts.plot(kind='bar', xlabel='url')\n",
    "_tmp = count_urls(no_suffix)\n",
    "top20 = _tmp.sort_values(by='counts').iloc[-20:]\n",
    "chart = alt.Chart(top20).mark_bar(size=20).encode(\n",
    "    x=alt.X('url', axis=alt.Axis(labelAngle=-30)).sort('-y'),\n",
    "    y=alt.Y('normalized_counts:Q', title='% of all URLs'),\n",
    "    text='url'\n",
    ").properties(width=600, height=200, title='Most Popular URLs (by domain)')\n",
    "display(chart)\n",
    "display(top20.iloc[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import forum_utils\n",
    "importlib.reload(forum_utils)\n",
    "from forum_utils import get_project_constants, about_utils \n",
    "\n",
    "latex_figs = get_project_constants(var='latex_figs')\n",
    "git_home = get_project_constants(var='git_home')\n",
    "latex_home = get_project_constants(var='latex_home')\n",
    "about_utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import altair as alt\n",
    "importlib.reload(alt)\n",
    "#tmp.sort_values(by='counts').iloc[-20:].counts.plot(kind='bar', xlabel='url')\n",
    "_tmp = count_urls(no_suffix)\n",
    "top20 = _tmp.sort_values(by='counts').iloc[-20:]\n",
    "\n",
    "base = alt.Chart(top20).encode(\n",
    "    #x=alt.X('url', axis=alt.Axis(labelAngle=-30), title='Domain').sort('-y'),\n",
    "    x=alt.X('url', axis=alt.Axis(labelAngle=-30), title='Domain',\n",
    "    sort=alt.EncodingSortField('normalized_counts', op='min', order='descending')),\n",
    "    #sort=alt.EncodingSortField(field='y:Q', op='min')),\n",
    "    y=alt.Y('normalized_counts:Q', title='% of all URLs', scale=alt.Scale(domain=[0,7], clamp=True)),\n",
    "    text='counts:Q'\n",
    ").mark_bar()\n",
    "chart = base.mark_bar(size=20).properties(width=650, height=200, title='20 Most Popular Domains') \\\n",
    "    + base.mark_text(dy=-5) \n",
    "\n",
    "#display(chart)\n",
    "\n",
    "#display(top20.iloc[::-1])\n",
    "# top20 + base.mark_text(align='left', dx=2)\n",
    "# First install NodeJS either by direct download or via a package manager, and then use the npm tool to install the required packages:\n",
    "# $ npm install vega-lite vega-cli canvas\n",
    "import altair_saver\n",
    "importlib.reload(altair_saver)\n",
    "chart = chart.configure(background='#F9F3DC').configure_view(fill='white')\n",
    "chart.save( str(Path(git_home, latex_figs, 'top_domains.pdf')),engine=\"altair_saver\")\n",
    "# https://stackoverflow.com/questions/66896225/altair-setting-raw-color-values-messes-up-sortfield\n",
    "# NOTE: if just use sort('y') get WARN Domains that should be unioned has conflicting sort properties. Sort will be set to true.\n",
    "# solution : use encodingsortfield with op 'sum' instead\n",
    "\n",
    "chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20.iloc[::-1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.round(2.32342344, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytextable\n",
    "\n",
    "top20['normalized_counts'] = top20.normalized_counts.apply(lambda x: np.round(x,2))\n",
    "pytextable.write(top20.iloc[::-1].to_numpy(), str(Path(git_home, latex_home, 'top_domains.tex')))\n",
    "\n",
    "display(top20.iloc[::-1])\n",
    "\n",
    "with open(Path(git_home, latex_home, 'top_domains.tex'), 'r') as f:\n",
    "    print(f.readlines())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str(Path(git_home, latex_fig, 'chart.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chart = alt.Chart(_tmp).mark_bar(size=20).encode(\n",
    "    x=alt.X('url', axis=alt.Axis(labelAngle=-30), sort=alt.EncodingSortField(field='-y')),\n",
    "    y=alt.Y('normalized_counts:Q', title='% of all URLs', scale=alt.Scale(domain=[0,7], clamp=True)),\n",
    "    text='url'\n",
    ").properties(width=600, height=200, title='Most Popular URLs (by domain)').configure(background='#F9F3DC').configure_view(fill='white')\n",
    "display(chart)\n",
    "display(top20.iloc[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(_tmp[_tmp.url == 'twitter.com'])\n",
    "_tmp[_tmp.url == 't.co']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
