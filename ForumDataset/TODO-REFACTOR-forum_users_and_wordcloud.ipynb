{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# only allow nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#$ ssh -N -L 8889:localhost:8889 chai@sffpc.local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from funcy import print_durations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nltk.org/book/ch05.html\n",
    "import nltk\n",
    "#from nltk import word_tokenize\n",
    "#nlp = spacy.load('en_core_web_lg')\n",
    "from nltk.probability import ConditionalFreqDist\n",
    "import gensim\n",
    "import gensim.downloader as api\n",
    "#from nltk import pos_tag\n",
    "\n",
    "# TODO: @redo decorator for only one variable\n",
    "# which prints time re-run\n",
    "\n",
    "# nltk.download( brown, universal_tagset)\n",
    "#_tagger = ConditionalFreqDist((w.lower(), t) for w, t in\n",
    "             #nltk.corpus.brown.tagged_words(tagset='universal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tagger = nltk.pos_tag\n",
    "with print_durations('importing models'):\n",
    "    model = api.load(\"glove-twitter-100\")\n",
    "# https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import funcy\n",
    "from funcy import silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#!pip install funcy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = nltk.Text(word.lower() for word in nltk.corpus.brown.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "# - Usage: replace df.apply() with parallel_apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(f'nogit_data/preprocessed_df.pd-1.5.1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write decorator that prints \\n if parameter: extra spacing\n",
    "# TODO: decorator that confirms or skips re-creating a variable (manually)\n",
    "def keep_nouns(query, model=None, tagger=None):\n",
    "    print(f'{query=}')\n",
    "    _similar_words = [word for word, score in model.wv.most_similar(query, topn=50)]\n",
    "    if not model:\n",
    "        model = api.load('glove-twitter-100') \n",
    "    if not tagger:\n",
    "        from nltk.corpus import brown\n",
    "        brown_tagged_sents = brown.tagged_sents()\n",
    "        brown_sents = brown.sents()\n",
    "        unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "    # okay go through and uh, find the most siilar, which has word and\n",
    "    # score\n",
    "    # dithc the score. have the similar words\n",
    "    # for each similar word run pos_tag\n",
    "\n",
    "    nouns = []\n",
    "    for word in _similar_words:\n",
    "        print(word)\n",
    "        word, tag = tagger.tag([word])[0]\n",
    "        if tag=='NN' or tag=='NP' or tag=='NPP':\n",
    "            nouns.append(word) \n",
    "    #nouns = [word, tag if tag=='NN' else None for (word, tag) in _tags]\n",
    "    #print(f'All similar tagged: {list(zip(_similar_words, _tags))}')\n",
    "    #print(f'All nouns: {nouns=}')\n",
    "    return nouns\n",
    "\n",
    "\n",
    "\n",
    "SETUP = True\n",
    "## setup:\n",
    "import nltk\n",
    "if SETUP:\n",
    "    from nltk.corpus import brown\n",
    "    brown_tagged_sents = brown.tagged_sents()\n",
    "    brown_sents = brown.sents()\n",
    "    import gensim.downloader as api\n",
    "    #my_model = api.load('glove-twitter-100') \n",
    "    print('loaded model')\n",
    "    print('trained unigram tagger')\n",
    "    my_unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "my_model = Word2Vec.load('nogit_data/tmp_word2vec.model')\n",
    "print(keep_nouns('concerned', model=my_model, tagger=my_unigram_tagger))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVE \n",
    " \n",
    "# # this still is trained on whole stentences, so 'test' will react to being used in verb vs noun form\n",
    "#from nltk.tokenize import word_tokenize\n",
    "#print(nltk.pos_tag(word_tokenize('forget the engine')))\n",
    "#print(nltk.pos_tag(word_tokenize('i forget what day it is ')))\n",
    "#print(nltk.pos_tag(word_tokenize('forget')))\n",
    "#from nltk.tag import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ARCHIVE \n",
    "\n",
    "#from nltk.corpus import brown\n",
    "#brown_tagged_sents = brown.tagged_sents()\n",
    "#brown_sents = brown.sents()\n",
    "# We train a UnigramTagger by specifying tagged sentence data as a parameter\n",
    "# when we initialize the tagger.\n",
    "#unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)\n",
    "\n",
    "# TESTS\n",
    "\n",
    "#unigram_tagger.tag(['forget'])\n",
    "#unigram_tagger.tag(['i', 'forget'])\n",
    "#unigram_tagger.tag(['i', 'gave', 'a', 'test'])\n",
    "#unigram_tagger.tag(['i', 'test', 'the', 'engine'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \n",
    "Unigram taggers are based on a simple statistical algorithm: for each token, assign the tag that is most likely for that particular token. For example, it will assign the tag JJ to any occurrence of the word frequent, since frequent is used as an adjective (e.g. a frequent word) more often than it is used as a verb (e.g. I frequent this cafe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(keep_nouns('worry', model=my_model, tagger=my_unigram_tagger))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordcloud for negative sentiment posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U tensorflow==2.10  # otherwise get keras module does not exist error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 April 2023\n",
    "# nrobot\n",
    "# Run the acornym expansion on the full dataset!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_short, remove_stopwords, strip_multiple_whitespaces\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_page_info = ['src_category_name', 'time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "\n",
    "columns_thread_info = ['src_category_name', 'thread_page_name', 'thread_page_num', 'thread_max_pages', 'thread_page_url']\n",
    "\n",
    "columns_likes = ['num_likers', 'likers']\n",
    "columns_quotes = ['num_quotes', 'quoted_post_ids', 'quoted_authors', 'quoted_contents']\n",
    "columns_authors = ['author', 'author_title', 'author_num_posts', 'author_num_reviews', 'author_url', 'join_date_readable', 'join_date_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp = pd.read_csv('nogit_data/list_of_post_contents.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: why are there NaNs in the author num posts?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = {}\n",
    "a[1] = 234\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = _df_tmp.dropna(subset=['time_downloaded', 'author_num_posts']).drop_duplicates('author')\n",
    "authors.author_num_posts = authors.author_num_posts.str.replace(',', '').astype(int)\n",
    "authors = authors[['author', 'author_num_posts', 'author_title', 'join_date_readable', 'author_url', 'post_text']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.sort_values(by='author_num_posts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors[authors.author_num_posts == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp[_df_tmp.author == 'bronsonkings'][['author', 'time_downloaded', 'author_num_posts', 'author_title', 'author_url', 'thread_page_url']].thread_page_url.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors.author.unique()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "import seaborn as sns\n",
    "#alt.Chart(authors).mark_boxplot().encode(\n",
    "    #alt.Y(\"author_num_posts\").scale(zero=False)\n",
    "#)\n",
    "plt.figure(figsize=(5,0.5))\n",
    "sns.boxplot(data=authors, x='author_num_posts', showfliers=False, width=0.7, ).set(title='IQR of # of Posts per Author (without outliers)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(data=authors, x='author_num_posts', color='r').set(alpha=1)\n",
    "sns.stripplot(data=authors, x='author_num_posts', size=3, edgecolor='gray',linewidth=0.8).set(\n",
    "    title='# of Posts per User, All Users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = authors.author_num_posts.describe()['mean']\n",
    "stddev = authors.author_num_posts.describe()['std']\n",
    "sns.violinplot(data=authors[authors.author_num_posts > (30.8 + 2*140.4)], x='author_num_posts', color='r', inner='quartile').set(alpha=1)\n",
    "sns.stripplot(data=authors[authors.author_num_posts > (30.8 + 2*140.4)], x='author_num_posts', size=3, edgecolor='gray',linewidth=0.8).set(\n",
    "    title=f'# of Posts per User \\n For users with > {mean+2*stddev:.1f} posts \\n (2 std devs ({stddev:.1f}) from mean ({mean:.1f}))')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 =  authors.author_num_posts.describe()['25%']\n",
    "q2 =  authors.author_num_posts.describe()['50%']\n",
    "q3 =  authors.author_num_posts.describe()['75%']\n",
    "print(f'Declaring outliers = 75% + 1.5 * (75% - 25%) = {q3} + 1.5 * ({q3} - {q1}) = {q3 + 1.5*(q3-q1)}')\n",
    "outliers = authors[authors.author_num_posts > (q3 + (1.5 * (q3 - q1)))]  \n",
    "outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.shape[0] / len(authors.author.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers.author_num_posts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(authors[authors.author_num_posts < 44.5].author_num_posts.sum() )\n",
    "authors[authors.author_num_posts < 44.5].author_num_posts.sum() /  outliers.author_num_posts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.author_num_posts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.author_num_posts.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.histplot(data=authors[authors.author_num_posts !=0], x=\"author_num_posts\", log_scale=True)#kde=True, \n",
    "sns.violinplot(data=authors[authors.author_num_posts !=0], x=\"author_num_posts\", log_scale=True)#kde=True, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%% time \n",
    "# Check \n",
    "raw_df = pd.read_csv('./nogit_data/list_of_post_contents.csv')\n",
    "len(raw_df.author.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#_df_tmp.dropna(subset=['time_downloaded'], inplace=True)\n",
    "\n",
    "_df_tmp[_df_tmp.author_num_posts.isna()]\n",
    "_df_tmp[_df_tmp.author == 'noblecrack']#.thread_page_url.values\n",
    "count_users_posts = _df_tmp.dropna(subset=['post_text'])\n",
    "count_users_posts[count_users_posts.author == 'sinyalekf'].author_num_posts.value_counts()\n",
    "count_users_posts[count_users_posts.author_num_posts.isna()]\n",
    "count_users_posts[count_users_posts.author == 'Harvala'].author_num_posts.values\n",
    "#count_users_posts[count_users_posts.author == 'Harvala'].thread_page_url.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp.sample(n=100).groupby('author').value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp.author.value_counts()\n",
    "_df_tmp.groupby('author').author_num_posts.sum() #/  _df_tmp.groupby('author').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp[_df_tmp.author == 'KittyHawk']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict \n",
    "user_data = defaultdict() \n",
    "for author in _df_tmp.author.sample(n=10):\n",
    "    user_data[author] = {}\n",
    "    user_data[author]['counted_num_posts'] = _df_tmp[_df_tmp.author == author].shape[0]\n",
    "    user_data[author]['html_num_posts'] = _df_tmp[_df_tmp.author == author].author_num_posts.dropna().iloc[0]#sort_values(by='author_num_posts').iloc[0].author_num_posts\n",
    "user_data = pd.DataFrame(user_data) \n",
    "user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp.dropna(subset='time_downloaded').drop_duplicates('author').author_num_posts.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_df_tmp.dropna(subset=['time_downloaded'], inplace=True)\n",
    "#_df_tmp.author_num_posts = df.author_num_posts.str.replace(',', '').astype(int)\n",
    "_df_tmp[_df_tmp.author_num_posts.isna()]\n",
    "_df_tmp[_df_tmp.author == 'noblecrack']#.thread_page_url.values\n",
    "count_users_posts = _df_tmp.dropna(subset=['post_text'])\n",
    "count_users_posts[count_users_posts.author == 'sinyalekf'].author_num_posts.value_counts()\n",
    "count_users_posts[count_users_posts.author_num_posts.isna()]\n",
    "count_users_posts[count_users_posts.author == 'Harvala'].author_num_posts.values\n",
    "#count_users_posts[count_users_posts.author == 'Harvala'].thread_page_url.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile='list_of_post_contents.csv', nrows=None, columns=None):\n",
    "    infile='list_of_post_contents.csv'\n",
    "\n",
    "    df = pd.read_csv(Path(os.getcwd(), 'nogit_data', infile), nrows=nrows)\n",
    "    print(f'{df.shape=}')\n",
    "    df.dropna(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "    df.drop_duplicates(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    print('\\n'.join(df.columns))\n",
    "    if not columns:\n",
    "        columns = ['src_category_name', 'time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "    print(f'keeping only: {columns=}')\n",
    "    df = df[columns]\n",
    "    if 'posted_date_readable' in columns:\n",
    "        df['posted_date_datetime'] = df.posted_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))\n",
    "    return df\n",
    "\n",
    "def get_discussions_only(df):\n",
    "    # remove posts that come from reviews (vs. discussions)\n",
    "    discussions = df[df.src_category_name.str.contains('Discussion')]\n",
    "    \n",
    "    # reformat 1,000 to 1000\n",
    "    #if discussions.author_num_posts.dtype != int:\n",
    "        #discussions.author_num_posts = discussions.author_num_posts.apply(lambda x: x.replace(',', ''))\n",
    "        #discussions.author_num_posts = discussions.author_num_posts.astype(int)\n",
    "    return discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot author information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data(columns=columns_authors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.author.unique().shape\n",
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['join_date_datetime'] = df.join_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.join_date_datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "# https://github.com/altair-viz/altair/issues/1021\n",
    "#def finalize_chart(chart):\n",
    "    #chart.x.axis = {'title': 'options'}\n",
    "    #chart.y.axis = {'title': 'percentage of students'}\n",
    "    #chart.mark = {'type': 'bar', 'size': 3)\n",
    "    #return chart\n",
    "\n",
    "alt.__version__ \n",
    "SCALE=1.5\n",
    "def theme_1(*args, **kwargs):\n",
    "    return {'width': 400, 'height': 300,\n",
    "            'config': {#'style': {'bar': {'size': 20}},\n",
    "                       #'legend': {'symbolSize': 20, 'titleFontSize': 20, 'labelFontSize': 20}, \n",
    "                       'title':{'fontSize': 20*SCALE},\n",
    "                       'axis': {'titleFontSize': 15*SCALE, 'labelFontSize': 15*SCALE},\n",
    "                        \"axisX\": { #https://towardsdatascience.com/consistently-beautiful-visualizations-with-altair-themes-c7f9f889602\n",
    "                                \"tickSize\": 10, # default, including it just to show you can change it\n",
    "                                \"tickWidth\":2,\n",
    "                        }\n",
    "            },\n",
    "    }\n",
    "            #'encoding': {'x': {'axis': {'title': 'options'}, 'scale': {'paddingOuter': 0.5, 'paddingInner': 0.5}},\n",
    "            #             'y': {'axis': {'title': 'percentage of students'}}}}\n",
    "alt.themes.register('theme_1', theme_1)\n",
    "alt.themes.enable('theme_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = df.copy()\n",
    "authors.drop_duplicates(subset=['author'], inplace=True)\n",
    "date_filter = (authors.join_date_datetime > '2000') & (authors.join_date_datetime < '2023')#.resample('Y').count().reset_index()\n",
    "daily = authors[date_filter].resample('D', on='join_date_datetime').apply({'author': 'count'}).reset_index()\n",
    "weekly = authors[date_filter].resample('W', on='join_date_datetime').apply({'author': 'count'}).reset_index()\n",
    "monthly = authors[date_filter].resample('M', on='join_date_datetime').apply({'author': 'count'}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.value_counts(subset='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily.sort_values(by='author')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alt.themes.enable('theme_1')\n",
    "#date_filter = alt.selection_interval\n",
    "\n",
    "bins = {'daily':daily, 'weekly':weekly, 'monthly':monthly}\n",
    "binwidth = {'daily':2, 'weekly':4, 'monthly':18}\n",
    "TIMEFREQ = 'weekly'\n",
    "my_df = bins[TIMEFREQ] \n",
    "# https://github.com/altair-viz/altair/issues/187\n",
    "date_min = my_df.join_date_datetime.min()\n",
    "date_max = my_df.join_date_datetime.max() \n",
    "\n",
    "\n",
    "chart = alt.Chart(my_df).mark_bar(\n",
    "    width=binwidth[TIMEFREQ],\n",
    "    opacity=1,\n",
    "    clip=True,\n",
    "    #line=True,\n",
    "    #stroke='gray',\n",
    "    #strokeOpacity=8\n",
    "    #color='lightblue'\n",
    "    ).encode(\n",
    "        alt.X('join_date_datetime:T',\n",
    "                scale=alt.Scale(nice={'interval': 'month', 'step': 1}, domain=[my_df.join_date_datetime.min(), my_df.join_date_datetime.max()]),\n",
    "                axis=alt.Axis(\n",
    "                    format={'date':'%x', 'month':'%b', 'year':'{%Y}', 'quarter':'%b.'},\n",
    "                    tickCount={\"interval\": \"month\", \"step\":1},\n",
    "                    labelAngle=-30,\n",
    "                    )\n",
    "                ).title('Time (Tick per Month)'),\n",
    "            alt.Y('author:Q',).title('Num. of Users'),\n",
    "            #color=alt.condition(genre_filter, alt.value(), alt.value(\"red\")),\n",
    "        ).properties(width=1200, height=250, title=f'User Join Date ({TIMEFREQ})')\n",
    "#chart.configure_bar(stroke='gray')\n",
    "\n",
    "\n",
    "years = []\n",
    "for year in range(2019,2023):\n",
    "    year_dt = pd.to_datetime(str(year)) # CAREFUL! must use str() here!\n",
    "    print(year_dt)\n",
    "    years.append(my_df[my_df.join_date_datetime >= year_dt].iloc[0].values[0])\n",
    "    \n",
    "rules = alt.Chart(pd.DataFrame({\n",
    "  'Date': years, \n",
    "  #'color': 'orange'\n",
    "})).mark_rule().encode(\n",
    "  x='Date:T',\n",
    "  color=alt.value('gray'),\n",
    "  #color=alt.Color('color:N', scale=None)\n",
    "      strokeWidth=alt.value(1.5)\n",
    ")\n",
    "#!pip install \n",
    "#(demo + rules )\n",
    "#(areas + chart)\n",
    "#(areas +  chart + rules)\n",
    "covid_start = my_df[my_df.join_date_datetime>= '2020-03-01'].iloc[0].values[0]\n",
    "covid_end = my_df[my_df.join_date_datetime>= '2020-07-31'].iloc[0].values[0]\n",
    "cutoff = pd.DataFrame({\n",
    "    'start': [covid_start],\n",
    "    'stop': [covid_end ]\n",
    "})\n",
    "\n",
    "area_covid = alt.Chart(\n",
    "    cutoff.reset_index()\n",
    ").mark_rect(\n",
    "    opacity=0.5,\n",
    "    stroke='red',\n",
    "    strokeOpacity=1,\n",
    "    strokeWidth=3\n",
    ").encode(\n",
    "    x='start',\n",
    "    x2='stop',\n",
    "    y=alt.value(0),  # pixels from top\n",
    "    y2=alt.value(250),  # pixels from top\n",
    "    color=alt.value('transparent'),\n",
    " \n",
    ")\n",
    "\n",
    "chart + rules\n",
    "\n",
    "final = (chart+rules).configure(background='#F9F3DC').configure_view(fill='white')\n",
    "final + area_covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors.author_num_posts.sort_values()\n",
    "authors.loc[83]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(df):\n",
    "\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "    #print(f'{my_stopwords=}')\n",
    "    #print(f'{df.columns=}')\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    CUSTOM_FILTERS = [\n",
    "        strip_tags, strip_punctuation, \n",
    "        lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "        lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "        lambda z: porter.stem(z, to_lowercase=False )\n",
    "    ]\n",
    "\n",
    "    df['preprocessed_posts'] = df['post_text'].parallel_apply(\n",
    "        lambda x: preprocess_string(x, CUSTOM_FILTERS)) \n",
    "    return df\n",
    "\n",
    "    #stop_nltk.extend([s.title() for s in stop_nltk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bigram_corpus(df, min_count=2, threshold=10): # TODO: consider taking in a phraser model directly, instead of params to pass to one\n",
    "    posts = df.preprocessed_posts.to_list()\n",
    "    my_phrases = gensim.models.Phrases(posts, min_count=2, threshold=threshold)\n",
    "    bigram_ifier = Phraser(my_phrases)\n",
    "\n",
    "    df['bigrammed_posts'] = df['preprocessed_posts'].progress_apply(\n",
    "        lambda post: bigram_ifier[post]) \n",
    "\n",
    "    bigrammed_corpus = df.bigrammed_posts.to_list()\n",
    "    print(f'Created word vectors for corpus size {len(bigrammed_corpus)=}, '\n",
    "          f'example post {bigrammed_corpus[0]=}')\n",
    "    return bigrammed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bigrams = create_bigram_corpus(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "with print_durations('loading data'):\n",
    "    df = load_data()#nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with print_durations('pre-processing text'):\n",
    "    df = nltk_preprocess(df)\n",
    "    df.to_pickle(f'./nogit_data/preprocessed_df.pd-{pd.__version__}.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.src_category_name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# word clouds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('nogit_data/preprocessed_df.pd-1.5.1.pkl')\n",
    "discussions = df[df.src_category_name.str.contains('Discussion')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with print_durations(''):\n",
    "    discussions['preprocessed_as_str'] = discussions.preprocessed_posts.progress_apply(lambda x: ' '.join(x))\n",
    "discussions.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "#for tag in ['Discusions', 'Reviews', '']:\n",
    "    #ives =  df_sample['preprocessed_as_str'][df_sample['sentiment_label'] == tag]\n",
    "ives =  discussions['post_text']\n",
    "my_wordcloud = WordCloud(width=1000, height=600, min_font_size=10, max_font_size=100, max_words=100, collocations=False, background_color=\"white\", colormap=\"viridis\").generate(str(ives))\n",
    "fig, ax = plt.subplots(figsize=(10,6)) # TODO: fix figsize to reasonable vals\n",
    "plt.title(f\"Wordcloud\")\n",
    "plt.imshow(my_wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "fig.patch.set_facecolor('#F9F3DC')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .... why is priesthood in there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_wordcloud.words_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anodomly same 1/10 (full = 180 mins, 1/10 = 18 mins)\n",
    "nrows = discussions.shape[0]\n",
    "df_sample = discussions.sample(int(nrows/100))\n",
    "df_sample = discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import option_context\n",
    "import numpy as np\n",
    "\n",
    "with option_context('display.max_colwidth', None):\n",
    "    display( df_sample.iloc[18])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "\n",
    "from transformers import pipeline\n",
    "sentiment_analysis = pipeline(model=\"finiteautomata/bertweet-base-sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "def silent_sentiment_analysis(x):\n",
    "    try:\n",
    "        return sentiment_analysis(x)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "sentiment = df_sample.post_text.progress_apply(lambda x: silent_sentiment_analysis(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.to_pickle('nogit_data/sentiment.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['sentiment'] = sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample[df_sample.sentiment.isna()].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(df_sample.sentiment.apply(lambda x: len(x) if x else None) != 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample['sentiment_label'] = df_sample.sentiment.apply(lambda x: x[0]['label'] if x else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sample.sentiment_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_sample[df_sample.sentiment_label == 'NEG'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_tmp = df_sample.sentiment_label.value_counts()\n",
    "_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "include_keywords = ['worri', 'worries', 'worried', 'Worry', 'Worri', 'Worries', 'Worried', 'Worrying', 'worrying', \n",
    "          'anxious', 'afraid', 'guilty', 'fear', 'worrying', 'Worrying']\n",
    "print(df_sample.sentiment_label.value_counts())\n",
    "import altair as al#t\n",
    "base = alt.Chart(df_sample).encode(color='sentiment_label', theta=alt.Theta('count(sentiment_label)',).stack(True))\n",
    "pie = base.mark_arc(outerRadius=120)\n",
    "text = base.mark_text(radius=130, size=15).encode(text=\"count(sentiment_label):Q\")\n",
    "pie+text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contains = df_sample.stack().str.contains('|'.join(include_keywords)).any(level=0)\n",
    "df_sample[contains].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from wordcloud import WordCloud\n",
    "_tmp = df_sample[contains]\n",
    "for tag in ['POS', 'NEG', 'NEU', None]:\n",
    "    #ives =  df_sample['preprocessed_as_str'][df_sample['sentiment_label'] == tag]\n",
    "    ives =  _tmp['preprocessed_as_str'][_tmp['sentiment_label'] == tag]\n",
    "    positive_wordcloud = WordCloud(max_font_size=80, max_words=100, background_color=\"white\").generate(str(ives))\n",
    "    plt.figure()\n",
    "    plt.title(f\"{tag=}itive Tweets - Wordcloud\")\n",
    "    plt.imshow(positive_wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_df_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
