{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!jupyter nbextension enable --py widgetsnbextension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 09 April 2023\n",
    "# nrobot\n",
    "# Run the acornym expansion on the full dataset!\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.parsing.preprocessing import strip_tags, strip_punctuation, strip_short, remove_stopwords, strip_multiple_whitespaces\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_page_info = ['time_downloaded', 'author', 'posted_date_readable',  'post_ordinal', 'thread_page_name', 'thread_page_num', 'thread_page_url', 'post_text']\n",
    "\n",
    "columns_thread_info = ['src_category_name', 'thread_page_name', 'thread_page_num', 'thread_max_pages', 'thread_page_url']\n",
    "\n",
    "columns_likes = ['num_likers', 'likers']\n",
    "columns_quotes = ['num_quotes', 'quoted_post_ids', 'quoted_authors', 'quoted_contents']\n",
    "columns_authors = ['author', 'author_title', 'author_num_posts', 'author_num_reviews', 'author_url', 'join_date_readable', 'join_date_data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(infile='list_of_post_contents.csv', nrows=None):\n",
    "    infile='list_of_post_contents.csv'\n",
    "\n",
    "    df = pd.read_csv(Path(os.getcwd(), 'nogit_data', infile), nrows=nrows)\n",
    "    print(f'{df.columns=}')\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    df.dropna(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "    df.drop_duplicates(subset=['post_text'], inplace=True)\n",
    "    print(f'{df.shape=}')\n",
    "\n",
    "    print(f'{columns_page_info=}')\n",
    "    df = df[columns_page_info]\n",
    "    df['posted_date_datetime'] = df.posted_date_readable.parallel_apply(\n",
    "        lambda x: pd.to_datetime(x))\n",
    "    return df\n",
    "\n",
    "def get_discussions_only(df):\n",
    "    # remove posts that come from reviews (vs. discussions)\n",
    "    discussions = df[df.src_category_name.str.contains('Discussion')]\n",
    "    # reformat 1,000 to 1000\n",
    "    if discussions.author_num_posts.dtype != int:\n",
    "        discussions.author_num_posts = discussions.author_num_posts.apply(lambda x: x.replace(',', ''))\n",
    "        discussions.author_num_posts = discussions.author_num_posts.astype(int)\n",
    "    return discussions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nltk_preprocess(df):\n",
    "\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    print(my_stopwords)\n",
    "    my_stopwords.extend([s.title() for s in my_stopwords])\n",
    "    print(f'{my_stopwords=}')\n",
    "    print(f'{df.columns=}')\n",
    "\n",
    "    porter = PorterStemmer()\n",
    "\n",
    "    CUSTOM_FILTERS = [\n",
    "        strip_tags, strip_punctuation, \n",
    "        lambda x: strip_short(x, minsize=2),  # remove only 1 letter words \n",
    "        lambda y: remove_stopwords(y, stopwords=my_stopwords),\n",
    "        lambda z: porter.stem(z, to_lowercase=False )\n",
    "    ]\n",
    "\n",
    "    df['preprocessed_posts'] = df['post_text'].parallel_apply(\n",
    "        lambda x: preprocess_string(x, CUSTOM_FILTERS)) \n",
    "    return df\n",
    "\n",
    "def create_word2vec(df, overwrite=False, outfile='nogit_data/Case_1/word2vec.bigrams.model'):\n",
    "\n",
    "    posts = df.preprocessed_posts.to_list()\n",
    "    my_phrases = gensim.models.Phrases(posts, min_count=1, threshold=10)\n",
    "    bigram_ifier = Phraser(my_phrases)\n",
    "\n",
    "    df['bigrammed_posts'] = df['preprocessed_posts'].parallel_apply(\n",
    "        lambda post: bigram_ifier[post]) \n",
    "\n",
    "    bigrammed_corpus = df.bigrammed_posts.to_list()\n",
    "    print(f'Creating word vectors for corpus size {len(bigrammed_corpus)=}, '\n",
    "          f'example post {bigrammed_corpus[0]=}')\n",
    "\n",
    "    model = Word2Vec(bigrammed_corpus, seed=42, workers=10)\n",
    "\n",
    "    if overwrite:\n",
    "            # Open \"path\" for writing, creating any parent directories as needed.\n",
    "        # TODO catch exception where parent folder doesn't exist; or \n",
    "        # force it to exist by committing to git (normally nogit_data ignored by git)\n",
    "        # os.makedirs(os.path.dirname(outfile), exist_ok=True)\n",
    "        model.save(outfile)\n",
    "        print(f'\\-- {overwrite=}, Saved model to {outfile=}')\n",
    "    return df, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_data()#nrows=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = load_data(nrows=1000)\n",
    "#df = load_data(nrows=10000)\n",
    "#df = load_data(None)\n",
    "df = nltk_preprocess(df)\n",
    "df, model = create_word2vec(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for query in ['SO']:#, 'MMS', 'HJ', 'HE', 'BJ', 'full_menu']:\n",
    "  sims = model.wv.most_similar(query, topn=20)  # get other similar words\n",
    "  #for item in sims:\n",
    "  print(f'{query=}\\t ' , ', '.join([ f'{word} = {vector:.2f}' for word, vector in sims]))\n",
    "  print('-')\n",
    "\n",
    "def find_abbreviation(query, model):\n",
    "    similar_words = model.wv.most_similar(query, topn=50)  # get other similar words\n",
    "    phrases = [word for word, vector in similar_words if '_' in word]\n",
    "    for phrase in phrases:\n",
    "        words = phrase.split('_')\n",
    "        inits = [w[0] for w in words]\n",
    "        candidate = ''.join(inits).upper()\n",
    "        if query.upper() == candidate:\n",
    "            print(f'{query=}, {phrase=}, Candidate: {phrase.replace(\"_\", \" \")}')\n",
    "            return\n",
    "    print(f'{query=}, no candidate found')\n",
    "\n",
    "find_abbreviation('TS', model)\n",
    "find_abbreviation('ST', model)\n",
    "find_abbreviation('BJ', model)\n",
    "find_abbreviation('HJ', model)\n",
    "find_abbreviation('FS', model)\n",
    "find_abbreviation('SO', model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('v3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "47339dd7c6d4bd32b6c40d54a37ff47f789dc7ff328ffb1f4a95f19daef7217e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
